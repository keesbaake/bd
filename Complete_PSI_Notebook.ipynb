{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSI Twitter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading of libraries\n",
    "\n",
    "import requests, os,re, csv, pixiedust\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from pixiedust import sc\n",
    "from dateutil import parser\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel, BisectingKMeans\n",
    "from numpy import array, dot, isnan\n",
    "from numpy.linalg import norm\n",
    "from itertools import groupby\n",
    "from math import log10, sqrt\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully enabled Spark Job Progress Monitor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pixiedust/utils/sparkJobProgressMonitor.py\", line 47, in startSparkJobProgressMonitor\n",
      "    progressMonitor = SparkJobProgressMonitor()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pixiedust/utils/sparkJobProgressMonitor.py\", line 174, in __init__\n",
      "    self.addSparkListener()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pixiedust/utils/sparkJobProgressMonitor.py\", line 203, in addSparkListener\n",
      "    _env.getTemplate(\"sparkJobProgressMonitor/addSparkListener.scala\").render()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2131, in run_cell_magic\n",
      "    result = fn(magic_arg_s, cell)\n",
      "  File \"<decorator-gen-126>\", line 2, in scala\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pixiedust/utils/scalaBridge.py\", line 183, in scala\n",
      "    self.getLineOption(line, \"channel\"), self.getLineOption(line, \"receiver\"))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pixiedust/utils/javaBridge.py\", line 51, in __init__\n",
      "    self.captureOutput(captureOutput)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pixiedust/utils/javaBridge.py\", line 71, in captureOutput\n",
      "    pixiedustOutputSink = JavaWrapper(\"com.ibm.pixiedust.PixiedustOutputStream\").jHandle(self.outputChannel)\n",
      "TypeError: 'JavaPackage' object is not callable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initializes the PixieDust monitor\n",
    "pixiedust.enableJobMonitor()\n",
    "sqlContext = pixiedust.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can quickly instantiatie the Spark Dataframe (used in part 2 and 3) from a pre-parsed .csv file in codeblock 1.6\n",
    "- Creation of the original Spark Dataframe is timeconsuming but is shown in parts 1.1 - 1.5 (also shows creation of .csv)\n",
    "- Parts 2 and 3 of this script uses the pre-parsed .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing of Twitter data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Initializing and loading of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests and os are used in the download_file function\n",
    "#dateutil is used to convert strings containing temporal data into datetime objects\n",
    "#pandas is used for easy queries on dataframes\n",
    "\n",
    "#downloads any file from a url to a specified location\n",
    "def download_file(url, location):\n",
    "    filename = os.path.join(location, url.split(\"/\")[-1].split(\"?\")[0])\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "            if chunk: \n",
    "                f.write(chunk)\n",
    "    return filename\n",
    "\n",
    "#removes unwanted characters found in certain tweets\n",
    "def sym_filter(string):\n",
    "    for char in [\"{\", \"}\", \"(\", \")\"]:\n",
    "        if string.count(char) == 1:\n",
    "            string = string.replace(char, \"\")\n",
    "    return string\n",
    "\n",
    "#loads the PSI_tweets.txt file as a list of list if already available locally\n",
    "def offline_PSI_tweets(n, txt_location = './PSI_tweets.txt'):    \n",
    "    i = 0\n",
    "    tweets = []\n",
    "    with open(txt_location) as txt_file:\n",
    "        for line in txt_file:\n",
    "            tweets += [line]\n",
    "            if i == (n-1):\n",
    "                return(tweets)\n",
    "            i += 1\n",
    "\n",
    "#checks whether expanding the join limit could find enable finding the next match  \n",
    "def possible_expand(og_match, next_match, string):\n",
    "    test = False\n",
    "    match = re.findall(pattern = (\".*\" + og_match + \" (.*),.*\" + next_match), string = string)\n",
    "    for i in range(2, 10):\n",
    "        if next_match in ('\"'.join(match[0].split('\"')[:i]) + '\"'):\n",
    "            test = True\n",
    "    return test\n",
    "\n",
    "\n",
    "#gives the (right) closest present variable found in the tweet relative to a given variable\n",
    "def check_next(og_element, check_list, tweet, og_index):\n",
    "    closest_list = []\n",
    "    copy_check_list = check_list[:]\n",
    "    copy_check_list.pop(copy_check_list.index(og_element))\n",
    "    \n",
    "    for element in copy_check_list:\n",
    "        if element in tweet:\n",
    "            for check in re.finditer((\"[ |{}]\" + element), tweet):\n",
    "                if check:\n",
    "                    if (og_index - (check.start() + 1)) < 0:\n",
    "                        closest_list += [[(og_index - check.start()), element]]  \n",
    "    if len(closest_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return pd.DataFrame(closest_list).loc[pd.DataFrame(closest_list)[0].idxmax()][1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Downloading of Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloads prehosted dataset from Dropbox\n",
    "url = 'https://www.dropbox.com/s/yz5biypudzjpc12/PSI_tweets.txt?dl=1'\n",
    "location = './' #relative location for Linux, saves it in the same folder as this script\n",
    "download_file(url, location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Collecting all tweets as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_RDD = sc.textFile('./PSI_tweets.txt')\n",
    "tweets = tweets_RDD.collect() #or tweets = tweets_RDD.take(500) for some testing \n",
    "\n",
    "#You can also use the offline_PSI_tweets() function to instatiate your data if its already downloaded\n",
    "## tweets = offline_PSI_tweets(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Extracing every variable from each Tweet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON structure was exploited to create a list of variables generally contained within each tweet, albeit in a different order. For each variable in our list, our filter would initially check whether this element was found within the tweet and would then determine the location of other present variables relative to the first one. Using each closest pair of variables as (fixed) outer bounds for each variable enables extraction of the data in between using regular expressions. Since the data for each variable could potentially contain literal representations of other variables (someone could tweet using the string \"probability:\"), the script splits up every tweet from the first variable onwards using comma’s (JSON's default delimitation character) and joins each part of the string until the second variable is found, an indication of the outer bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produces a list of list containing all variables in the check_list per tweet\n",
    "#The multi_list list accounts for variables of the same name (tweet_id and profile_id are both called id for example)\n",
    "#The count of multi_list (containing already processed variables) is used to indicate the right regex match\n",
    "\n",
    "check_list = ['sentiment:', 'metadata:', 'probability:', 'possibly_sensitive:', 'in_reply_to_user_id_str:', 'created_at:', 'truncated:', 'source:', 'retweet_count:', 'retweeted:', 'in_reply_to_screen_name:', 'id_str:', 'in_reply_to_user_id:', 'id:', 'text:', 'lang:', 'class:', 'favorited:', 'utc_offset:', 'friends_count:', 'profile_image_url_https:', 'profile_background_image_url:', 'listed_count:', 'default_profile_image:', 'favourites_count:', 'is_translator:', 'description:', 'created_at:', 'profile_background_image_url_https:', 'protected:', 'screen_name:', 'profile_link_color:', 'id_str:', 'is_translation_enabled:', 'translator_type:', 'geo_enabled:', 'profile_background_color:', 'id:', 'lang:', 'has_extended_profile:', 'profile_sidebar_border_color:', 'profile_text_color:', 'verified:', 'profile_image_url:', 'time_zone:', 'url:', 'contributors_enabled:', 'profile_banner_url:', 'entities:', 'statuses_count:', 'default_profile:', 'followers_count:', 'profile_use_background_image:', 'name:', 'location:', 'profile_sidebar_fill_color:', 'gtype:', 'bbox:', 'latitude:', 'type:', 'longitude:']\n",
    "parsed_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    multi_list = []\n",
    "    row = [None] * len(check_list)\n",
    "    for i in range(len(check_list)):\n",
    "        \n",
    "        if len(re.findall( (\"[ |{}]\" + check_list[i]), string=tweet))  == 0:\n",
    "            row[i] = None\n",
    "            multi_list += [check_list[i]]\n",
    "\n",
    "        if len(re.findall( (\"[ |{}]\" + check_list[i]), string=tweet)) > 0:\n",
    "            index_list = []\n",
    "            for check in re.finditer((\"[ |{}]\" + check_list[i]), string=tweet):\n",
    "                if check:\n",
    "                    index_list += [check.start() + 1]\n",
    "\n",
    "            og_index = index_list[multi_list.count(check_list[i])]\n",
    "            next_match = check_next(og_element = check_list[i], check_list=check_list, tweet = tweet, og_index= og_index)\n",
    "            x = 0\n",
    "\n",
    "            if next_match:\n",
    "                match = re.findall(pattern = (\".*\" + check_list[i] + \" (.*),.*\" + next_match), string = tweet)\n",
    "                if len(match) > 0:\n",
    "                    if possible_expand(check_list[i], next_match, tweet): \n",
    "                        while next_match not in ('\"'.join(match[0].split('\"')[:x]) + '\"'):\n",
    "                            x += 1\n",
    "                        row[i] = [('\"'.join(match[0].split('\"')[:(x-1)]) + '\"')]\n",
    "                        multi_list += [check_list[i]]\n",
    "\n",
    "                    elif next_match in match[0]:\n",
    "                        while not next_match in \",\".join(match[0].split(\",\")[:x]):\n",
    "                            x += 1\n",
    "                        row[i] = [\",\".join(match[0].split(\",\")[:(x -1)])]\n",
    "                        multi_list += [check_list[i]]\n",
    "                    elif not next_match in match[0]:\n",
    "                        row[i] = match[0]\n",
    "                        \n",
    "            elif (i == (len(check_list) -1) and check_list[-1] in tweet):\n",
    "                match = re.findall(pattern = (\".*\" + check_list[-1] + \" (.*)[}|)][}|)]\"), string = tweet)\n",
    "                if len(match) == 1:\n",
    "                    row[i] = match[0]\n",
    "                else:\n",
    "                    row[i] = None\n",
    "                    \n",
    "\n",
    "        else:\n",
    "            row[i] = None\n",
    "            multi_list += [check_list[i]]    \n",
    "    parsed_tweets += [row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Filtering unrequired colums and imposing a fixed structure for the remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a list of lists (data) that only contains the wanted colums:\n",
    "#'sentiment:, probability:, possibly_sensitive:, in_reply_to_user_id_str:, created_at:, in_reply_to_screen_name:, id_str:, in_reply_to_user_id:, id:, text:, lang:, class:, utc_offset:, description:, created_at:, screen_name:, geo_enabled:, lang:, time_zone:, name:, location:, latitude:, longitude:'\n",
    "\n",
    "data = []\n",
    "\n",
    "for tweet in parsed_tweets:\n",
    "    filtered_check_list = []\n",
    "    line = []\n",
    "    for i in range(len(tweet)):\n",
    "        if i in [0, 2, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16, 18, 26, 27, 30, 35, 38, 44, 53, 54, 58, 60]:\n",
    "            line += [tweet[i]]\n",
    "            filtered_check_list += [check_list[i]]\n",
    "    data += [line]\n",
    "    \n",
    "#Set any data found to None if length == 0 (indicating null values), all data is still of type string\n",
    "for tweet in data:\n",
    "    for i in range(len(tweet)):\n",
    "        if tweet[i]:\n",
    "            if len(tweet[i]) == 0:\n",
    "                tweet[i] = None\n",
    "                \n",
    "#If the join limit was expanded in the first step, the joined values are still in a list of len(1), these are unpacked in thi sstep\n",
    "for x in range(3):\n",
    "    for tweet in data:\n",
    "        for i in range(len(tweet)):\n",
    "            if tweet[i]:\n",
    "                if type(tweet[i]) == list:\n",
    "                    if len(tweet[i]) == 1:\n",
    "                        tweet[i] = tweet[i][0]\n",
    "                        try:\n",
    "                            tweet[i] = eval(tweet[i])\n",
    "                        except:\n",
    "                            tweet[i] = tweet[i]\n",
    "\n",
    "                            \n",
    "#Explictely sets the data type for each column as it needs to be fixed to be able to convert it to a Spark Dataframe\n",
    "for tweet in data:\n",
    "    for i in range(len(tweet)):\n",
    "        #0\n",
    "        if i == 0:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = [int(eval(tweet[i])[0]), int(eval(tweet[i])[1]), int(eval(tweet[i])[2]), eval(tweet[i])[3]]\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "        #1\n",
    "        if i == 1:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = float(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "        #2            \n",
    "        if i == 2:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = eval(sym_filter(tweet[i]).title())\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "        #3            \n",
    "        if i == 3:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = int(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "        #4            \n",
    "        if i == 4:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = parser.parse(eval(tweet[i]))\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "        #5            \n",
    "        if i == 5:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = str(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "        #6            \n",
    "        if i == 6:\n",
    "            #tweet[i] = \"Temp_fill\"\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = int(eval(tweet[i]))\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "        #7            \n",
    "        if i == 7:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = int(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "        #8            \n",
    "        if i == 8:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = int(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "\n",
    "        #9            \n",
    "        if i == 9:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = str(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "\n",
    "        #10            \n",
    "        if i == 10:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = str(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "        #11            \n",
    "        if i == 11:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = str(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "\n",
    "        #12            \n",
    "        if i == 12:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = int(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "\n",
    "        #13            \n",
    "        if i == 13:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = str(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "        #14            \n",
    "        if i == 14:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = parser.parse(eval(tweet[i]))\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "\n",
    "        #15            \n",
    "        if i == 15:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = str(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "\n",
    "        #16\n",
    "        if i == 16:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = eval(sym_filter(tweet[i]).title())\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "\n",
    "        #17            \n",
    "        if i == 17:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = str(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "\n",
    "        #18            \n",
    "        if i == 18:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = str(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "        #19            \n",
    "        if i == 19:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = str(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "        #20            \n",
    "        if i == 20:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = str(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "        #21\n",
    "        if i == 21:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = float(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None\n",
    "\n",
    "\n",
    "        #22\n",
    "        if i == 22:\n",
    "            if tweet[i]:\n",
    "                try:\n",
    "                    tweet[i] = float(tweet[i])\n",
    "                except:\n",
    "                    tweet[i] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (opt). Converting the data to Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you ran the upper code, the data can be directly converted to a Spark DF\n",
    "#Else, you can just use a pre-parsed .csv file available in the next codeblock\n",
    "tweetDF = sqlContext.createDataFrame(data, filtered_check_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (opt). Writing the parsed list containing all Tweets to a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writes the result to csv file\n",
    "import csv\n",
    "with open(\"parsed_tweets.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(filtered_check_list)\n",
    "    for row in data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Instantiating the Spark Dataframe from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./parsed_tweets.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Upper code was used to produce a .csv file containing all parsed Twitter data\n",
    "#Was uploaded to Dropbox and made available via a public link, lower function downloads that file\n",
    "\n",
    "url = \"https://www.dropbox.com/s/he5tvdxriqj9s9b/parsed_tweets.csv?dl=1\"\n",
    "location = './'\n",
    "download_file(url, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a csv_tweets list containing all parsed data (see upper codeblocks)\n",
    "csv_tweets = []\n",
    "\n",
    "with open(\"./parsed_tweets.csv\") as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter = \",\")\n",
    "    headers = next(readCSV)\n",
    "    for row in readCSV:\n",
    "        csv_tweets += [row] \n",
    "\n",
    "#Since Python imports CSV segments as strings, the fixed structure has to be imposed again\n",
    "#Evaluates 3 times to remove all nested quotation marks (\"'text'\")\n",
    "for x in range(3):\n",
    "    for tweet in csv_tweets:\n",
    "        for i in range(len(tweet)):\n",
    "\n",
    "            if len(str(tweet[i])) == 0:\n",
    "                tweet[i] = None\n",
    "            try:\n",
    "                tweet[i] = eval(tweet[i])       \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                tweet[i] = parser.parse(tweet[i])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "#Imposes and evaluates fixed data types for all columns    \n",
    "for tweet in csv_tweets:\n",
    "    for i in range(len(tweet)):\n",
    "        if tweet[i] is not None:\n",
    "            if tweet[i] == Ellipsis:\n",
    "                tweet[i] = None\n",
    "\n",
    "            if i in [5, 9, 10, 11, 13, 15, 17, 18, 19, 20]:\n",
    "                try:\n",
    "                    tweet[i] = str(tweet[i])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            if i == 0:\n",
    "                try:\n",
    "                    tweet[i] = [int(tweet[i][0]), int(tweet[i][1]), int(tweet[i][2]), str(tweet[i][3])] \n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            if i in [1, 21, 22]:\n",
    "                try:\n",
    "                    tweet[i] = float(tweet[i])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            if i in [2, 16]:\n",
    "                try:\n",
    "                    tweet[i] = eval(tweet[i].title())\n",
    "                except:\n",
    "                    pass\n",
    "            if i in [4, 14]:\n",
    "\n",
    "                try:\n",
    "                    tweet[i] = parser.parse(tweet[i])\n",
    "                except:\n",
    "                    pass\n",
    "            if i == 6:\n",
    "                try:\n",
    "                    tweet[i] = int(tweet[i])\n",
    "                except:\n",
    "                    pass\n",
    "            if i in [3, 7, 8, 12]:\n",
    "\n",
    "                try:\n",
    "                    tweet[i] = int(tweet[i])\n",
    "                except:\n",
    "                    pass\n",
    "            if i == 5:\n",
    "\n",
    "                try:\n",
    "                    tweet[i] = str(tweet[i])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "\n",
    "#Creates a Spark Dataframe from the list parsed from the premade CSV file\n",
    "tweetDF_csv = sqlContext.createDataFrame(csv_tweets, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Finding Clusters Using Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create empty list for saving all the coordinates of tweets\n",
    "coordinates= []\n",
    "\n",
    "## open .csv file as file, only extract coordiantes\n",
    "with open('./parsed_tweets.csv',newline='') as file:\n",
    "    csv_headers = next(file)\n",
    "    for i in csv.reader(file):\n",
    "        coordinates+= [[i[-2],i[-1]]]\n",
    "        \n",
    "coordinates = coordinates[1:]\n",
    "\n",
    "\n",
    "## create RDD object containing all the coordinates\n",
    "data = sc.parallelize(coordinates)\n",
    "\n",
    "### filter out coordinates which are empty and also convert string coordinates to float value\n",
    "coordinates_process = data.filter(lambda x : (len(x[0]) != 0) and (len(x[1])!=0)).map(lambda x : (float(x[0]),float(x[1]))).map(lambda x : (Vectors.dense(x),))\n",
    "\n",
    "## take first to see the result\n",
    "print(coordinates_process.take(1))\n",
    "\n",
    "\n",
    "## convert RDD to spark dataframe\n",
    "df = spark.createDataFrame(coordinates_process, [\"features\"])\n",
    "## give an impression what does it look like\n",
    "df.show()\n",
    "\n",
    "## create empty cost list\n",
    "cost_all = []\n",
    "\n",
    "##specify the K value range which will be used to find optimal k cluster\n",
    "k = range(2,100,2)\n",
    "\n",
    "## try every k value , and calculate cost for corresponding cost ,then save costs into empty cost list\n",
    "for i in k:\n",
    "    kmeans = KMeans(k=i, seed=1)\n",
    "    model = kmeans.fit(df)\n",
    "    cost = model.computeCost(df)###type(cost) is float\n",
    "    cost_all = cost_all+[cost]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "## every k and its cost value \n",
    "plt.plot(k,cost_all)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('K-cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try to find the k value where the cost starts stable\n",
    "index = []\n",
    "for i in range(len(cost_all)-1):\n",
    "    if (cost_all[i]-cost_all[i+1]) < 0.1:\n",
    "        index +=[i]\n",
    "index_final = index[0]\n",
    "\n",
    "## get the optimal k value\n",
    "k_final = k[index_final]\n",
    "print('perfect k is :',k_final)\n",
    "\n",
    "\n",
    "## use the optimal k value and run model again \n",
    "kmeans = KMeans(k=k_final, seed=1)\n",
    "model = kmeans.fit(df)\n",
    "##retrieve the cluster centers\n",
    "centers = model.clusterCenters()\n",
    "## get cost value\n",
    "cost_final = model.computeCost(df)###type(cost) is float\n",
    "print('cost_final = ',cost_final)\n",
    "print(centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert all the features and their labels(which cluster they belong to) to pandas.dataframe\n",
    "transformed = model.transform(df).select(\"features\", \"prediction\")\n",
    "pdf = transformed.toPandas()\n",
    "### separate coordiantes\n",
    "coor = [ [i[0],i[1]] for i in pdf['features']]\n",
    "### extract labels \n",
    "label = pdf['prediction']\n",
    "for i in range(len(coor)):\n",
    "    coor[i] = coor[i]+[label[i]]\n",
    "\n",
    "center = pd.DataFrame(centers,columns = ['lat','lon'])\n",
    "dataframe = pd.DataFrame(coor,columns = ['lat','lon','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print culstered result \n",
    "plt.scatter(dataframe['lat'], dataframe['lon'],c = dataframe['label'],cmap=plt.cm.Paired)\n",
    "plt.scatter(center['lat'],center['lon'],marker='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Twitter Text Content Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load the data and pre-process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment:</th>\n",
       "      <th>probability:</th>\n",
       "      <th>possibly_sensitive:</th>\n",
       "      <th>in_reply_to_user_id_str:</th>\n",
       "      <th>created_at:</th>\n",
       "      <th>in_reply_to_screen_name:</th>\n",
       "      <th>id_str:</th>\n",
       "      <th>in_reply_to_user_id:</th>\n",
       "      <th>id:</th>\n",
       "      <th>text:</th>\n",
       "      <th>...</th>\n",
       "      <th>description:</th>\n",
       "      <th>created_at:.1</th>\n",
       "      <th>screen_name:</th>\n",
       "      <th>geo_enabled:</th>\n",
       "      <th>lang:.1</th>\n",
       "      <th>time_zone:</th>\n",
       "      <th>name:</th>\n",
       "      <th>location:</th>\n",
       "      <th>latitude:</th>\n",
       "      <th>longitude:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, -2, 0, 'long']</td>\n",
       "      <td>2.627796e-06</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-10-09 02:38:13+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.849460e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.681141e+07</td>\n",
       "      <td>Good day on the burn. @ Mount Shavano Trailhea...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"I'm an avid outdoorsman originally from Sprin...</td>\n",
       "      <td>2016-10-09 02:38:13+00:00</td>\n",
       "      <td>\"CragSquatch\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Atlantic Time (Canada)\"</td>\n",
       "      <td>\"Alec Villarreal\"</td>\n",
       "      <td>\"Durango, CO\"</td>\n",
       "      <td>38.595941</td>\n",
       "      <td>-106.199650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 'long']</td>\n",
       "      <td>9.053528e-07</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-06-21 15:01:11+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.775419e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.332052e+08</td>\n",
       "      <td>Serenity. @ Mt. Shavano And Tabeguache https:/...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"be patient. work hard. stay humble. NWU soccer.\"</td>\n",
       "      <td>2017-06-21 15:01:11+00:00</td>\n",
       "      <td>\"rymian\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Central Time (US &amp; Canada)\"</td>\n",
       "      <td>\"ryan anderson\"</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>38.596673</td>\n",
       "      <td>-106.196988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2, 0, 1, 'long']</td>\n",
       "      <td>9.536176e-08</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-06-25 03:14:41+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.788137e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.571868e+08</td>\n",
       "      <td>Aspen trees make me happy. @ Mt. Shavano And T...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"An epic failure who has received a love that ...</td>\n",
       "      <td>2017-06-25 03:14:41+00:00</td>\n",
       "      <td>\"revrickydean\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>\"Ricky Jones\"</td>\n",
       "      <td>\"Tulsa, OK\"</td>\n",
       "      <td>38.596673</td>\n",
       "      <td>-106.196988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 'long']</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-04 03:43:59+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.599769e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.517352e+07</td>\n",
       "      <td>2 mile afternoon hike at high elevation. @ Mt....</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Father, outdoor enthusiast, food and beverage...</td>\n",
       "      <td>2017-05-04 03:43:59+00:00</td>\n",
       "      <td>\"patrickau\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Mountain Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Patrick Payne\"</td>\n",
       "      <td>\"Salida, Colorado\"</td>\n",
       "      <td>38.596673</td>\n",
       "      <td>-106.196988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 0, 0, 'long']</td>\n",
       "      <td>1.609444e-10</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-04 10:59:04+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.600864e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.517352e+07</td>\n",
       "      <td>Hiking the Colorado Trail @ Mt. Shavano And Ta...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Father, outdoor enthusiast, food and beverage...</td>\n",
       "      <td>2017-05-04 10:59:04+00:00</td>\n",
       "      <td>\"patrickau\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Mountain Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Patrick Payne\"</td>\n",
       "      <td>\"Salida, Colorado\"</td>\n",
       "      <td>38.596673</td>\n",
       "      <td>-106.196988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0, 0, 0, 'long']</td>\n",
       "      <td>1.052048e-09</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-14 02:56:17+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.635888e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.517352e+07</td>\n",
       "      <td>Sleepy #camping @ Mt. Shavano And Tabeguache h...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Father, outdoor enthusiast, food and beverage...</td>\n",
       "      <td>2017-05-14 02:56:17+00:00</td>\n",
       "      <td>\"patrickau\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Mountain Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Patrick Payne\"</td>\n",
       "      <td>\"Salida, Colorado\"</td>\n",
       "      <td>38.596673</td>\n",
       "      <td>-106.196988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0, 0, 0, 'long']</td>\n",
       "      <td>1.041908e-05</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-14 12:06:47+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.637273e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.517352e+07</td>\n",
       "      <td>Just posted a photo @ Mt. Shavano And Tabeguac...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Father, outdoor enthusiast, food and beverage...</td>\n",
       "      <td>2017-05-14 12:06:47+00:00</td>\n",
       "      <td>\"patrickau\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Mountain Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Patrick Payne\"</td>\n",
       "      <td>\"Salida, Colorado\"</td>\n",
       "      <td>38.596673</td>\n",
       "      <td>-106.196988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0, 0, 0, 'long']</td>\n",
       "      <td>1.041908e-05</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-14 12:11:14+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.637284e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.517352e+07</td>\n",
       "      <td>Just posted a photo @ Mt. Shavano And Tabeguac...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Father, outdoor enthusiast, food and beverage...</td>\n",
       "      <td>2017-05-14 12:11:14+00:00</td>\n",
       "      <td>\"patrickau\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Mountain Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Patrick Payne\"</td>\n",
       "      <td>\"Salida, Colorado\"</td>\n",
       "      <td>38.596673</td>\n",
       "      <td>-106.196988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[2, -1, 1, 'short']</td>\n",
       "      <td>1.129235e-09</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-07 01:22:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.063077e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.517352e+07</td>\n",
       "      <td>Christmas tree permit! #christmastreehunting @...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Father, outdoor enthusiast, food and beverage...</td>\n",
       "      <td>2016-12-07 01:22:00+00:00</td>\n",
       "      <td>\"patrickau\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Mountain Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Patrick Payne\"</td>\n",
       "      <td>\"Salida, Colorado\"</td>\n",
       "      <td>38.594561</td>\n",
       "      <td>-106.198577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[2, -1, 1, 'short']</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-04 03:42:42+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.599766e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.517352e+07</td>\n",
       "      <td>Afternoon hike with moose viewing! @ Shavano/T...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Father, outdoor enthusiast, food and beverage...</td>\n",
       "      <td>2017-05-04 03:42:42+00:00</td>\n",
       "      <td>\"patrickau\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Mountain Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Patrick Payne\"</td>\n",
       "      <td>\"Salida, Colorado\"</td>\n",
       "      <td>38.594561</td>\n",
       "      <td>-106.198577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.997989e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-10 23:39:34+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.863316e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.368285e+08</td>\n",
       "      <td>Just posted a photo @ Billings Lake https://t....</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Avid adventurer of all summer and winter spor...</td>\n",
       "      <td>2016-01-10 23:39:34+00:00</td>\n",
       "      <td>\"MTB_RockDude\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Pacific Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Andrew Elliott\"</td>\n",
       "      <td>\"Buena Vista, CO\"</td>\n",
       "      <td>38.623607</td>\n",
       "      <td>-106.330580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.631238e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-07-28 19:14:59+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.587426e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.517352e+07</td>\n",
       "      <td>Packing it out this morning. @ Billings Lake h...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Father, outdoor enthusiast, food and beverage...</td>\n",
       "      <td>2016-07-28 19:14:59+00:00</td>\n",
       "      <td>\"patrickau\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Mountain Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Patrick Payne\"</td>\n",
       "      <td>\"Salida, Colorado\"</td>\n",
       "      <td>38.623607</td>\n",
       "      <td>-106.330580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0, 0, 0, 'long']</td>\n",
       "      <td>5.898917e-10</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-03 11:31:48+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.608003e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.517352e+07</td>\n",
       "      <td>So went into an old tunnel yesterday. When I e...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Father, outdoor enthusiast, food and beverage...</td>\n",
       "      <td>2016-08-03 11:31:48+00:00</td>\n",
       "      <td>\"patrickau\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Mountain Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Patrick Payne\"</td>\n",
       "      <td>\"Salida, Colorado\"</td>\n",
       "      <td>38.623607</td>\n",
       "      <td>-106.330580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0, -2, -1, 'long']</td>\n",
       "      <td>8.554724e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-08 02:07:43+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.884670e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.246420e+08</td>\n",
       "      <td>Off work. On way home for dinner.  Thinking so...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>2013-01-08 02:07:43+00:00</td>\n",
       "      <td>\"MikeUSNRet\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Michael Wenman\"</td>\n",
       "      <td>\"Buena Vista, Colorado\"</td>\n",
       "      <td>38.516421</td>\n",
       "      <td>-106.077239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0, 0, 0, 'long']</td>\n",
       "      <td>7.145626e-05</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-29 18:53:42+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.822543e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.892625e+07</td>\n",
       "      <td>Feet recovery on the road home.  http://t.co/z...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Retired Air Force officer who runs lots of ma...</td>\n",
       "      <td>2015-03-29 18:53:42+00:00</td>\n",
       "      <td>\"yardbird1964\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Mountain Time (US &amp; Canada)</td>\n",
       "      <td>\"Steve Bremner\"</td>\n",
       "      <td>\"Manitou Springs, Colorado\"</td>\n",
       "      <td>38.539595</td>\n",
       "      <td>-106.188343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[1, -1, 0, 'short']</td>\n",
       "      <td>4.260928e-06</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-06-26 01:56:02+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.791563e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.398739e+07</td>\n",
       "      <td>#mtshavano @ Cross Roads Church https://t.co/b...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Doing my best to tell people God is not mad a...</td>\n",
       "      <td>2017-06-26 01:56:02+00:00</td>\n",
       "      <td>\"josephochambers\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>\"Joe Chambers\"</td>\n",
       "      <td>\"Buena Vista, CO\"</td>\n",
       "      <td>38.521827</td>\n",
       "      <td>-106.078961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[5, -2, 1, 'long']</td>\n",
       "      <td>2.075258e-07</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-03-14 23:18:01+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.417906e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.681141e+07</td>\n",
       "      <td>Sweet first day at the mountain with @_cody567...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"I'm an avid outdoorsman originally from Sprin...</td>\n",
       "      <td>2017-03-14 23:18:01+00:00</td>\n",
       "      <td>\"CragSquatch\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Atlantic Time (Canada)\"</td>\n",
       "      <td>\"Alec Villarreal\"</td>\n",
       "      <td>\"Durango, CO\"</td>\n",
       "      <td>38.520637</td>\n",
       "      <td>-106.079676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.999965e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-11 21:24:05+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.080598e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.240907e+08</td>\n",
       "      <td>Great morning @monarchmountain thanks for the ...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"We are located in Central Colorado in the hea...</td>\n",
       "      <td>2016-12-11 21:24:05+00:00</td>\n",
       "      <td>\"WoodsDistillery\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>\"Wood's HM Distillery\"</td>\n",
       "      <td>\"144 W 1st Salida Co\"</td>\n",
       "      <td>38.520637</td>\n",
       "      <td>-106.079676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-12 22:30:36+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.196730e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.240907e+08</td>\n",
       "      <td>And the epicness continues, ridiculous COD lap...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"We are located in Central Colorado in the hea...</td>\n",
       "      <td>2017-01-12 22:30:36+00:00</td>\n",
       "      <td>\"WoodsDistillery\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>\"Wood's HM Distillery\"</td>\n",
       "      <td>\"144 W 1st Salida Co\"</td>\n",
       "      <td>38.520637</td>\n",
       "      <td>-106.079676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0, 0, 0, 'long']</td>\n",
       "      <td>7.368363e-07</td>\n",
       "      <td>False</td>\n",
       "      <td>18768691.0</td>\n",
       "      <td>2017-01-20 19:12:28+00:00</td>\n",
       "      <td>MonarchMountain</td>\n",
       "      <td>8.225222e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.225222e+17</td>\n",
       "      <td>@monarchmountain had another 6\" overnight, tha...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"We are located in Central Colorado in the hea...</td>\n",
       "      <td>2017-01-20 19:12:28+00:00</td>\n",
       "      <td>\"WoodsDistillery\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>\"Wood's HM Distillery\"</td>\n",
       "      <td>\"144 W 1st Salida Co\"</td>\n",
       "      <td>38.520637</td>\n",
       "      <td>-106.079676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.820079e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-24 21:03:39+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.239997e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.240907e+08</td>\n",
       "      <td>Ski.I.A. investigating yet another epic #pow d...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"We are located in Central Colorado in the hea...</td>\n",
       "      <td>2017-01-24 21:03:39+00:00</td>\n",
       "      <td>\"WoodsDistillery\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>\"Wood's HM Distillery\"</td>\n",
       "      <td>\"144 W 1st Salida Co\"</td>\n",
       "      <td>38.520637</td>\n",
       "      <td>-106.079676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.999987e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-02-01 15:54:49+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.268211e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.240907e+08</td>\n",
       "      <td>AM rewards #skibikepaddlewhiskey #sunrise #col...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"We are located in Central Colorado in the hea...</td>\n",
       "      <td>2017-02-01 15:54:49+00:00</td>\n",
       "      <td>\"WoodsDistillery\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>\"Wood's HM Distillery\"</td>\n",
       "      <td>\"144 W 1st Salida Co\"</td>\n",
       "      <td>38.520637</td>\n",
       "      <td>-106.079676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.999998e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-02-21 16:23:28+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.340761e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.240907e+08</td>\n",
       "      <td>Pearl looking for groomers as the sunrise figh...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"We are located in Central Colorado in the hea...</td>\n",
       "      <td>2017-02-21 16:23:28+00:00</td>\n",
       "      <td>\"WoodsDistillery\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>\"Wood's HM Distillery\"</td>\n",
       "      <td>\"144 W 1st Salida Co\"</td>\n",
       "      <td>38.520637</td>\n",
       "      <td>-106.079676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-04-16 21:33:33+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.537231e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.240907e+08</td>\n",
       "      <td>Mike\"Mosh Pit\" Reed killing the closing day co...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"We are located in Central Colorado in the hea...</td>\n",
       "      <td>2017-04-16 21:33:33+00:00</td>\n",
       "      <td>\"WoodsDistillery\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>\"Wood's HM Distillery\"</td>\n",
       "      <td>\"144 W 1st Salida Co\"</td>\n",
       "      <td>38.520637</td>\n",
       "      <td>-106.079676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[1, 0, 1, 'long']</td>\n",
       "      <td>5.751085e-05</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-03-13 01:56:09+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.411056e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.517352e+07</td>\n",
       "      <td>Post snowmobile ride. Hot chocolate and apple ...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Father, outdoor enthusiast, food and beverage...</td>\n",
       "      <td>2017-03-13 01:56:09+00:00</td>\n",
       "      <td>\"patrickau\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Mountain Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Patrick Payne\"</td>\n",
       "      <td>\"Salida, Colorado\"</td>\n",
       "      <td>38.520637</td>\n",
       "      <td>-106.079676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.999980e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-11-16 19:30:28+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.340660e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.240907e+08</td>\n",
       "      <td>Epic!  #powder#freshie @ Monarch Mountain http...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"We are located in Central Colorado in the hea...</td>\n",
       "      <td>2014-11-16 19:30:28+00:00</td>\n",
       "      <td>\"WoodsDistillery\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>\"Wood's HM Distillery\"</td>\n",
       "      <td>\"144 W 1st Salida Co\"</td>\n",
       "      <td>38.536800</td>\n",
       "      <td>-106.180128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-12-17 20:58:59+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.453223e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.240907e+08</td>\n",
       "      <td>Dr Wood diagnosing the sickness that kept us o...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"We are located in Central Colorado in the hea...</td>\n",
       "      <td>2014-12-17 20:58:59+00:00</td>\n",
       "      <td>\"WoodsDistillery\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>\"Wood's HM Distillery\"</td>\n",
       "      <td>\"144 W 1st Salida Co\"</td>\n",
       "      <td>38.536800</td>\n",
       "      <td>-106.180128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[1, -3, -1, 'short']</td>\n",
       "      <td>5.254930e-08</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-12-17 21:02:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.453230e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.240907e+08</td>\n",
       "      <td>Cooper and I were the first up Mirkwood on the...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"We are located in Central Colorado in the hea...</td>\n",
       "      <td>2014-12-17 21:02:00+00:00</td>\n",
       "      <td>\"WoodsDistillery\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>\"Wood's HM Distillery\"</td>\n",
       "      <td>\"144 W 1st Salida Co\"</td>\n",
       "      <td>38.536800</td>\n",
       "      <td>-106.180128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0, -2, -1, 'long']</td>\n",
       "      <td>1.019689e-07</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-11-25 22:35:15+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.373740e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.240907e+08</td>\n",
       "      <td>The Ski.I.A investigating the lack of skiers i...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"We are located in Central Colorado in the hea...</td>\n",
       "      <td>2014-11-25 22:35:15+00:00</td>\n",
       "      <td>\"WoodsDistillery\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>\"Wood's HM Distillery\"</td>\n",
       "      <td>\"144 W 1st Salida Co\"</td>\n",
       "      <td>38.534321</td>\n",
       "      <td>-106.144861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.999711e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-12-04 17:13:52+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.405546e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.240907e+08</td>\n",
       "      <td>Pre-whiskey distillation laps monarchmountain!...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"We are located in Central Colorado in the hea...</td>\n",
       "      <td>2014-12-04 17:13:52+00:00</td>\n",
       "      <td>\"WoodsDistillery\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>\"Wood's HM Distillery\"</td>\n",
       "      <td>\"144 W 1st Salida Co\"</td>\n",
       "      <td>38.534321</td>\n",
       "      <td>-106.144861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12756</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12757</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12758</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12759</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-09-17 19:32:56+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.919449e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.919449e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Greasy fingers on the keyboard\"</td>\n",
       "      <td>2010-09-17 19:32:56+00:00</td>\n",
       "      <td>\"be_weeeeee\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Ben West\"</td>\n",
       "      <td>\"Dillon, CO\"</td>\n",
       "      <td>39.502002</td>\n",
       "      <td>-106.150997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12760</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-09-15 00:09:20+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.084828e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.193182e+08</td>\n",
       "      <td>Playing at Blu's tonight feeling from 8-10pm#n...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Musician\"</td>\n",
       "      <td>2017-09-15 00:09:20+00:00</td>\n",
       "      <td>\"nsteingart\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>\"steingart@gmail.com\"</td>\n",
       "      <td>\"Vail, CO \"</td>\n",
       "      <td>39.631029</td>\n",
       "      <td>-106.290523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12761</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-09-17 00:24:16+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.092114e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.193182e+08</td>\n",
       "      <td>I'm at Blu's tonight from 8-10pm!! @ Blu's Res...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Musician\"</td>\n",
       "      <td>2017-09-17 00:24:16+00:00</td>\n",
       "      <td>\"nsteingart\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>\"steingart@gmail.com\"</td>\n",
       "      <td>\"Vail, CO \"</td>\n",
       "      <td>39.631029</td>\n",
       "      <td>-106.290523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12762</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-06-18 21:47:17+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.765570e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.537449e+08</td>\n",
       "      <td>If you wanna see what shit hits the fan after ...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"SEU20 ⚽️ #15\"</td>\n",
       "      <td>2017-06-18 21:47:17+00:00</td>\n",
       "      <td>\"kayleightoupal\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Kayleigh Toupal\"</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>39.630966</td>\n",
       "      <td>-106.290313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12763</th>\n",
       "      <td>[2, -1, 1, 'short']</td>\n",
       "      <td>6.823771e-03</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-21 16:32:51+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.793198e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.566697e+07</td>\n",
       "      <td>Off to climb a mountain! #colorado #14er http:...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Day job with @ErgonBike. Play job with @TeamT...</td>\n",
       "      <td>2015-03-21 16:32:51+00:00</td>\n",
       "      <td>\"JeffKerkove\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Mountain Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Jeff Kerkove\"</td>\n",
       "      <td>\"Eagle, CO\"</td>\n",
       "      <td>39.535257</td>\n",
       "      <td>-106.142588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12764</th>\n",
       "      <td>[1, -1, 0, '1']</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-06-04 19:47:45+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.391819e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-06-04 19:47:45+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12765</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-07-30 18:42:05+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.156375e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.156375e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Cook, Travel, Marketer @NextGen Healthcare, B...</td>\n",
       "      <td>2009-07-30 18:42:05+00:00</td>\n",
       "      <td>\"lawrencester\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"catie lawrence\"</td>\n",
       "      <td>\"Colorado \"</td>\n",
       "      <td>39.543088</td>\n",
       "      <td>-106.119953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12766</th>\n",
       "      <td>[2, -1, 1, 'short']</td>\n",
       "      <td>3.039903e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-09-17 20:45:31+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.095187e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.558359e+09</td>\n",
       "      <td>Winter is coming! @ Copper Mountain Ski Area h...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Accomplished freelance CFO, strategic consult...</td>\n",
       "      <td>2017-09-17 20:45:31+00:00</td>\n",
       "      <td>\"TheFitCFO\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Mountain Time (US &amp; Canada)\"</td>\n",
       "      <td>\"Mark Johnson\"</td>\n",
       "      <td>\"Colorado\"</td>\n",
       "      <td>39.500779</td>\n",
       "      <td>-106.153743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12767</th>\n",
       "      <td>[1, -1, 0, 'short']</td>\n",
       "      <td>1.170454e-03</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-02 22:09:33+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.623723e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.407886e+08</td>\n",
       "      <td>Spent Saturday at #CopperMountain with some sn...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>2015-02-02 22:09:33+00:00</td>\n",
       "      <td>\"xxpl0re\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>America/Boise</td>\n",
       "      <td>\"Kyle Personett\"</td>\n",
       "      <td>\"Colorado\"</td>\n",
       "      <td>39.500385</td>\n",
       "      <td>-106.156559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12768</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.999981e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-16 00:38:48+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.772677e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.407886e+08</td>\n",
       "      <td>Blue bird day @coppermtn with mountainboy95 #v...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>2015-03-16 00:38:48+00:00</td>\n",
       "      <td>\"xxpl0re\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>America/Boise</td>\n",
       "      <td>\"Kyle Personett\"</td>\n",
       "      <td>\"Colorado\"</td>\n",
       "      <td>39.500382</td>\n",
       "      <td>-106.156582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12769</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.999426e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-04 17:34:45+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.517939e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-04 17:34:45+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12770</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-01-20 19:03:42+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.407886e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.407886e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>2011-01-20 19:03:42+00:00</td>\n",
       "      <td>\"xxpl0re\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>America/Boise</td>\n",
       "      <td>\"Kyle Personett\"</td>\n",
       "      <td>\"Colorado\"</td>\n",
       "      <td>39.500151</td>\n",
       "      <td>-106.155966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12771</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-07-17 02:28:24+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.545030e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.041670e+07</td>\n",
       "      <td>The babes and I in Copper for mac&amp;amp;cheese f...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Whiskey shooter. Hopeless dreamer. Midnight w...</td>\n",
       "      <td>2016-07-17 02:28:24+00:00</td>\n",
       "      <td>\"ohthatbagel\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>\"Bethanie\"</td>\n",
       "      <td>\"Denver, CO\"</td>\n",
       "      <td>39.500227</td>\n",
       "      <td>-106.156081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12772</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-07-17 02:32:15+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.545040e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.041670e+07</td>\n",
       "      <td>Charlotte, Freya, and I enjoying sunshine, mac...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Whiskey shooter. Hopeless dreamer. Midnight w...</td>\n",
       "      <td>2016-07-17 02:32:15+00:00</td>\n",
       "      <td>\"ohthatbagel\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>\"Bethanie\"</td>\n",
       "      <td>\"Denver, CO\"</td>\n",
       "      <td>39.500227</td>\n",
       "      <td>-106.156081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12773</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-07-17 02:37:20+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.545052e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.041670e+07</td>\n",
       "      <td>Mac and cheese! Hints of green chile, smoked b...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Whiskey shooter. Hopeless dreamer. Midnight w...</td>\n",
       "      <td>2016-07-17 02:37:20+00:00</td>\n",
       "      <td>\"ohthatbagel\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>\"Bethanie\"</td>\n",
       "      <td>\"Denver, CO\"</td>\n",
       "      <td>39.500227</td>\n",
       "      <td>-106.156081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12774</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.991959e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-07-17 02:51:27+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.545088e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.041670e+07</td>\n",
       "      <td>❤️ @ Copper Mountain https://t.co/7ui2r73fMI</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Whiskey shooter. Hopeless dreamer. Midnight w...</td>\n",
       "      <td>2016-07-17 02:51:27+00:00</td>\n",
       "      <td>\"ohthatbagel\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>\"Bethanie\"</td>\n",
       "      <td>\"Denver, CO\"</td>\n",
       "      <td>39.500227</td>\n",
       "      <td>-106.156081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12775</th>\n",
       "      <td>[3, 0, 1, 'long']</td>\n",
       "      <td>2.063013e-08</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-04-08 15:26:51+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.184601e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.407886e+08</td>\n",
       "      <td>Yesterday was a good time to skip some class, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>2016-04-08 15:26:51+00:00</td>\n",
       "      <td>\"xxpl0re\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>America/Boise</td>\n",
       "      <td>\"Kyle Personett\"</td>\n",
       "      <td>\"Colorado\"</td>\n",
       "      <td>39.500379</td>\n",
       "      <td>-106.156654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12776</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-10-20 14:24:29+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.564761e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-10-20 14:24:29+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12777</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-01-20 19:03:42+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.407886e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.407886e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>2011-01-20 19:03:42+00:00</td>\n",
       "      <td>\"xxpl0re\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>America/Boise</td>\n",
       "      <td>\"Kyle Personett\"</td>\n",
       "      <td>\"Colorado\"</td>\n",
       "      <td>39.500384</td>\n",
       "      <td>-106.156596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12778</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.994707e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-17 03:24:04+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.646829e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.204670e+08</td>\n",
       "      <td>Impact @ Blue Lakes, Colorado https://t.co/J7d...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"always down for an adventure. insta: rathbuner\"</td>\n",
       "      <td>2017-05-17 03:24:04+00:00</td>\n",
       "      <td>\"joelrathbun1\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Joel Rathbun\"</td>\n",
       "      <td>\"Wichita\"</td>\n",
       "      <td>39.383879</td>\n",
       "      <td>-106.068049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12779</th>\n",
       "      <td>[1, -1, 0, 'short']</td>\n",
       "      <td>7.091284e-08</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-15 17:30:18+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.641711e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.154097e+08</td>\n",
       "      <td>When your living life as an adventure 10,000 f...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Game Changer ✨ Rule Breaker | Biz Development...</td>\n",
       "      <td>2017-05-15 17:30:18+00:00</td>\n",
       "      <td>\"denisebuchman\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>\"Denise Buchman ☕️\"</td>\n",
       "      <td>\"United States\"</td>\n",
       "      <td>39.479822</td>\n",
       "      <td>-106.045828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12780</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.999781e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-15 15:43:11+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.641441e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.641441e+17</td>\n",
       "      <td>\"We sat in front of probability the two dumbes...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-15 15:43:11+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12781</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-05-03 01:21:54+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.395663e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.395663e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-05-03 01:21:54+00:00</td>\n",
       "      <td>\"SimonJC_\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>\"Simon John Cheng\"</td>\n",
       "      <td>\"Las Vegas, Nevada\"</td>\n",
       "      <td>39.184329</td>\n",
       "      <td>-105.963198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12782</th>\n",
       "      <td>[1, -1, 0, 'short']</td>\n",
       "      <td>1.639308e-04</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-22 23:44:35+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.668020e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.977403e+08</td>\n",
       "      <td>Exploring nature with the family round 2 #Scen...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Snapchat: Newskitron   Train to become the ve...</td>\n",
       "      <td>2017-05-22 23:44:35+00:00</td>\n",
       "      <td>\"J_NewSki\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>\"Central Time (US &amp; Canada)\"</td>\n",
       "      <td>\"James Pinewski\"</td>\n",
       "      <td>\"Pallet Town, Minnesota\"</td>\n",
       "      <td>39.577744</td>\n",
       "      <td>-106.129562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12783</th>\n",
       "      <td>[3, -1, 1, 'short']</td>\n",
       "      <td>1.206906e-07</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-16 01:46:31+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.642960e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.891358e+07</td>\n",
       "      <td>Getting to enjoy a little vaca in the Colorado...</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Singing and a picking in the Old Crow Medicin...</td>\n",
       "      <td>2017-05-16 01:46:31+00:00</td>\n",
       "      <td>\"chancemccoy\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Chance McCoy\"</td>\n",
       "      <td>\"Nashville, TN\"</td>\n",
       "      <td>38.842391</td>\n",
       "      <td>-106.119731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12784</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-19 23:40:58+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.657139e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.657139e+17</td>\n",
       "      <td>\"Rad picture from our friend Audrey at Spring ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-19 23:40:58+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12785</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-09 19:12:19+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.496397e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.496397e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-09 19:12:19+00:00</td>\n",
       "      <td>\"The_Moon_Life\"</td>\n",
       "      <td>True</td>\n",
       "      <td>\"en\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"The Moon Life\"</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>38.843933</td>\n",
       "      <td>-106.123290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12786 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sentiment:  probability: possibly_sensitive:  \\\n",
       "0        [3, -2, 0, 'long']  2.627796e-06               False   \n",
       "1         [0, 0, 0, 'long']  9.053528e-07               False   \n",
       "2         [2, 0, 1, 'long']  9.536176e-08               False   \n",
       "3         [0, 0, 0, 'long']  1.000000e+00               False   \n",
       "4         [0, 0, 0, 'long']  1.609444e-10               False   \n",
       "5         [0, 0, 0, 'long']  1.052048e-09               False   \n",
       "6         [0, 0, 0, 'long']  1.041908e-05               False   \n",
       "7         [0, 0, 0, 'long']  1.041908e-05               False   \n",
       "8       [2, -1, 1, 'short']  1.129235e-09               False   \n",
       "9       [2, -1, 1, 'short']  1.000000e+00               False   \n",
       "10                      NaN  9.997989e-01               False   \n",
       "11                      NaN  5.631238e-01               False   \n",
       "12        [0, 0, 0, 'long']  5.898917e-10               False   \n",
       "13      [0, -2, -1, 'long']  8.554724e-03                 NaN   \n",
       "14        [0, 0, 0, 'long']  7.145626e-05               False   \n",
       "15      [1, -1, 0, 'short']  4.260928e-06               False   \n",
       "16       [5, -2, 1, 'long']  2.075258e-07               False   \n",
       "17                      NaN  9.999965e-01               False   \n",
       "18                      NaN  1.000000e+00               False   \n",
       "19        [0, 0, 0, 'long']  7.368363e-07               False   \n",
       "20                      NaN  9.820079e-01               False   \n",
       "21                      NaN  9.999987e-01               False   \n",
       "22                      NaN  9.999998e-01               False   \n",
       "23                      NaN  1.000000e+00               False   \n",
       "24        [1, 0, 1, 'long']  5.751085e-05               False   \n",
       "25                      NaN  9.999980e-01               False   \n",
       "26                      NaN  1.000000e+00               False   \n",
       "27     [1, -3, -1, 'short']  5.254930e-08               False   \n",
       "28      [0, -2, -1, 'long']  1.019689e-07               False   \n",
       "29                      NaN  9.999711e-01               False   \n",
       "...                     ...           ...                 ...   \n",
       "12756                   NaN           NaN                 NaN   \n",
       "12757                   NaN           NaN                 NaN   \n",
       "12758                   NaN           NaN                 NaN   \n",
       "12759                   NaN           NaN                 NaN   \n",
       "12760                   NaN  1.000000e+00               False   \n",
       "12761                   NaN  1.000000e+00               False   \n",
       "12762                   NaN  1.000000e+00               False   \n",
       "12763   [2, -1, 1, 'short']  6.823771e-03               False   \n",
       "12764       [1, -1, 0, '1']  1.000000e+00               False   \n",
       "12765                   NaN           NaN                 NaN   \n",
       "12766   [2, -1, 1, 'short']  3.039903e-02               False   \n",
       "12767   [1, -1, 0, 'short']  1.170454e-03               False   \n",
       "12768                   NaN  9.999981e-01               False   \n",
       "12769                   NaN  9.999426e-01               False   \n",
       "12770                   NaN           NaN                 NaN   \n",
       "12771                   NaN  1.000000e+00               False   \n",
       "12772                   NaN  1.000000e+00               False   \n",
       "12773                   NaN  1.000000e+00               False   \n",
       "12774                   NaN  9.991959e-01               False   \n",
       "12775     [3, 0, 1, 'long']  2.063013e-08               False   \n",
       "12776                   NaN  9.999999e-01               False   \n",
       "12777                   NaN           NaN                 NaN   \n",
       "12778                   NaN  9.994707e-01               False   \n",
       "12779   [1, -1, 0, 'short']  7.091284e-08               False   \n",
       "12780                   NaN  9.999781e-01                 NaN   \n",
       "12781                   NaN           NaN                 NaN   \n",
       "12782   [1, -1, 0, 'short']  1.639308e-04               False   \n",
       "12783   [3, -1, 1, 'short']  1.206906e-07               False   \n",
       "12784                   NaN  9.999999e-01               False   \n",
       "12785                   NaN           NaN                 NaN   \n",
       "\n",
       "       in_reply_to_user_id_str:                created_at:  \\\n",
       "0                           NaN  2016-10-09 02:38:13+00:00   \n",
       "1                           NaN  2017-06-21 15:01:11+00:00   \n",
       "2                           NaN  2017-06-25 03:14:41+00:00   \n",
       "3                           NaN  2017-05-04 03:43:59+00:00   \n",
       "4                           NaN  2017-05-04 10:59:04+00:00   \n",
       "5                           NaN  2017-05-14 02:56:17+00:00   \n",
       "6                           NaN  2017-05-14 12:06:47+00:00   \n",
       "7                           NaN  2017-05-14 12:11:14+00:00   \n",
       "8                           NaN  2016-12-07 01:22:00+00:00   \n",
       "9                           NaN  2017-05-04 03:42:42+00:00   \n",
       "10                          NaN  2016-01-10 23:39:34+00:00   \n",
       "11                          NaN  2016-07-28 19:14:59+00:00   \n",
       "12                          NaN  2016-08-03 11:31:48+00:00   \n",
       "13                          NaN  2013-01-08 02:07:43+00:00   \n",
       "14                          NaN  2015-03-29 18:53:42+00:00   \n",
       "15                          NaN  2017-06-26 01:56:02+00:00   \n",
       "16                          NaN  2017-03-14 23:18:01+00:00   \n",
       "17                          NaN  2016-12-11 21:24:05+00:00   \n",
       "18                          NaN  2017-01-12 22:30:36+00:00   \n",
       "19                   18768691.0  2017-01-20 19:12:28+00:00   \n",
       "20                          NaN  2017-01-24 21:03:39+00:00   \n",
       "21                          NaN  2017-02-01 15:54:49+00:00   \n",
       "22                          NaN  2017-02-21 16:23:28+00:00   \n",
       "23                          NaN  2017-04-16 21:33:33+00:00   \n",
       "24                          NaN  2017-03-13 01:56:09+00:00   \n",
       "25                          NaN  2014-11-16 19:30:28+00:00   \n",
       "26                          NaN  2014-12-17 20:58:59+00:00   \n",
       "27                          NaN  2014-12-17 21:02:00+00:00   \n",
       "28                          NaN  2014-11-25 22:35:15+00:00   \n",
       "29                          NaN  2014-12-04 17:13:52+00:00   \n",
       "...                         ...                        ...   \n",
       "12756                       NaN                        NaN   \n",
       "12757                       NaN                        NaN   \n",
       "12758                       NaN                        NaN   \n",
       "12759                       NaN  2010-09-17 19:32:56+00:00   \n",
       "12760                       NaN  2017-09-15 00:09:20+00:00   \n",
       "12761                       NaN  2017-09-17 00:24:16+00:00   \n",
       "12762                       NaN  2017-06-18 21:47:17+00:00   \n",
       "12763                       NaN  2015-03-21 16:32:51+00:00   \n",
       "12764                       NaN  2016-06-04 19:47:45+00:00   \n",
       "12765                       NaN  2009-07-30 18:42:05+00:00   \n",
       "12766                       NaN  2017-09-17 20:45:31+00:00   \n",
       "12767                       NaN  2015-02-02 22:09:33+00:00   \n",
       "12768                       NaN  2015-03-16 00:38:48+00:00   \n",
       "12769                       NaN  2015-01-04 17:34:45+00:00   \n",
       "12770                       NaN  2011-01-20 19:03:42+00:00   \n",
       "12771                       NaN  2016-07-17 02:28:24+00:00   \n",
       "12772                       NaN  2016-07-17 02:32:15+00:00   \n",
       "12773                       NaN  2016-07-17 02:37:20+00:00   \n",
       "12774                       NaN  2016-07-17 02:51:27+00:00   \n",
       "12775                       NaN  2016-04-08 15:26:51+00:00   \n",
       "12776                       NaN  2015-10-20 14:24:29+00:00   \n",
       "12777                       NaN  2011-01-20 19:03:42+00:00   \n",
       "12778                       NaN  2017-05-17 03:24:04+00:00   \n",
       "12779                       NaN  2017-05-15 17:30:18+00:00   \n",
       "12780                       NaN  2017-05-15 15:43:11+00:00   \n",
       "12781                       NaN  2010-05-03 01:21:54+00:00   \n",
       "12782                       NaN  2017-05-22 23:44:35+00:00   \n",
       "12783                       NaN  2017-05-16 01:46:31+00:00   \n",
       "12784                       NaN  2017-05-19 23:40:58+00:00   \n",
       "12785                       NaN  2013-06-09 19:12:19+00:00   \n",
       "\n",
       "      in_reply_to_screen_name:       id_str:  in_reply_to_user_id:  \\\n",
       "0                          NaN  7.849460e+17                   NaN   \n",
       "1                          NaN  8.775419e+17                   NaN   \n",
       "2                          NaN  8.788137e+17                   NaN   \n",
       "3                          NaN  8.599769e+17                   NaN   \n",
       "4                          NaN  8.600864e+17                   NaN   \n",
       "5                          NaN  8.635888e+17                   NaN   \n",
       "6                          NaN  8.637273e+17                   NaN   \n",
       "7                          NaN  8.637284e+17                   NaN   \n",
       "8                          NaN  8.063077e+17                   NaN   \n",
       "9                          NaN  8.599766e+17                   NaN   \n",
       "10                         NaN  6.863316e+17                   NaN   \n",
       "11                         NaN  7.587426e+17                   NaN   \n",
       "12                         NaN  7.608003e+17                   NaN   \n",
       "13                         NaN  2.884670e+17                   NaN   \n",
       "14                         NaN  5.822543e+17                   NaN   \n",
       "15                         NaN  8.791563e+17                   NaN   \n",
       "16                         NaN  8.417906e+17                   NaN   \n",
       "17                         NaN  8.080598e+17                   NaN   \n",
       "18                         NaN  8.196730e+17                   NaN   \n",
       "19             MonarchMountain  8.225222e+17                   NaN   \n",
       "20                         NaN  8.239997e+17                   NaN   \n",
       "21                         NaN  8.268211e+17                   NaN   \n",
       "22                         NaN  8.340761e+17                   NaN   \n",
       "23                         NaN  8.537231e+17                   NaN   \n",
       "24                         NaN  8.411056e+17                   NaN   \n",
       "25                         NaN  5.340660e+17                   NaN   \n",
       "26                         NaN  5.453223e+17                   NaN   \n",
       "27                         NaN  5.453230e+17                   NaN   \n",
       "28                         NaN  5.373740e+17                   NaN   \n",
       "29                         NaN  5.405546e+17                   NaN   \n",
       "...                        ...           ...                   ...   \n",
       "12756                      NaN           NaN                   NaN   \n",
       "12757                      NaN           NaN                   NaN   \n",
       "12758                      NaN           NaN                   NaN   \n",
       "12759                      NaN  1.919449e+08                   NaN   \n",
       "12760                      NaN  9.084828e+17                   NaN   \n",
       "12761                      NaN  9.092114e+17                   NaN   \n",
       "12762                      NaN  8.765570e+17                   NaN   \n",
       "12763                      NaN  5.793198e+17                   NaN   \n",
       "12764                      NaN  7.391819e+17                   NaN   \n",
       "12765                      NaN  6.156375e+07                   NaN   \n",
       "12766                      NaN  9.095187e+17                   NaN   \n",
       "12767                      NaN  5.623723e+17                   NaN   \n",
       "12768                      NaN  5.772677e+17                   NaN   \n",
       "12769                      NaN  5.517939e+17                   NaN   \n",
       "12770                      NaN  2.407886e+08                   NaN   \n",
       "12771                      NaN  7.545030e+17                   NaN   \n",
       "12772                      NaN  7.545040e+17                   NaN   \n",
       "12773                      NaN  7.545052e+17                   NaN   \n",
       "12774                      NaN  7.545088e+17                   NaN   \n",
       "12775                      NaN  7.184601e+17                   NaN   \n",
       "12776                      NaN  6.564761e+17                   NaN   \n",
       "12777                      NaN  2.407886e+08                   NaN   \n",
       "12778                      NaN  8.646829e+17                   NaN   \n",
       "12779                      NaN  8.641711e+17                   NaN   \n",
       "12780                      NaN  8.641441e+17                   NaN   \n",
       "12781                      NaN  1.395663e+08                   NaN   \n",
       "12782                      NaN  8.668020e+17                   NaN   \n",
       "12783                      NaN  8.642960e+17                   NaN   \n",
       "12784                      NaN  8.657139e+17                   NaN   \n",
       "12785                      NaN  1.496397e+09                   NaN   \n",
       "\n",
       "                id:                                              text:  \\\n",
       "0      6.681141e+07  Good day on the burn. @ Mount Shavano Trailhea...   \n",
       "1      3.332052e+08  Serenity. @ Mt. Shavano And Tabeguache https:/...   \n",
       "2      2.571868e+08  Aspen trees make me happy. @ Mt. Shavano And T...   \n",
       "3      2.517352e+07  2 mile afternoon hike at high elevation. @ Mt....   \n",
       "4      2.517352e+07  Hiking the Colorado Trail @ Mt. Shavano And Ta...   \n",
       "5      2.517352e+07  Sleepy #camping @ Mt. Shavano And Tabeguache h...   \n",
       "6      2.517352e+07  Just posted a photo @ Mt. Shavano And Tabeguac...   \n",
       "7      2.517352e+07  Just posted a photo @ Mt. Shavano And Tabeguac...   \n",
       "8      2.517352e+07  Christmas tree permit! #christmastreehunting @...   \n",
       "9      2.517352e+07  Afternoon hike with moose viewing! @ Shavano/T...   \n",
       "10     7.368285e+08  Just posted a photo @ Billings Lake https://t....   \n",
       "11     2.517352e+07  Packing it out this morning. @ Billings Lake h...   \n",
       "12     2.517352e+07  So went into an old tunnel yesterday. When I e...   \n",
       "13     1.246420e+08  Off work. On way home for dinner.  Thinking so...   \n",
       "14     1.892625e+07  Feet recovery on the road home.  http://t.co/z...   \n",
       "15     6.398739e+07  #mtshavano @ Cross Roads Church https://t.co/b...   \n",
       "16     6.681141e+07  Sweet first day at the mountain with @_cody567...   \n",
       "17     6.240907e+08  Great morning @monarchmountain thanks for the ...   \n",
       "18     6.240907e+08  And the epicness continues, ridiculous COD lap...   \n",
       "19     8.225222e+17  @monarchmountain had another 6\" overnight, tha...   \n",
       "20     6.240907e+08  Ski.I.A. investigating yet another epic #pow d...   \n",
       "21     6.240907e+08  AM rewards #skibikepaddlewhiskey #sunrise #col...   \n",
       "22     6.240907e+08  Pearl looking for groomers as the sunrise figh...   \n",
       "23     6.240907e+08  Mike\"Mosh Pit\" Reed killing the closing day co...   \n",
       "24     2.517352e+07  Post snowmobile ride. Hot chocolate and apple ...   \n",
       "25     6.240907e+08  Epic!  #powder#freshie @ Monarch Mountain http...   \n",
       "26     6.240907e+08  Dr Wood diagnosing the sickness that kept us o...   \n",
       "27     6.240907e+08  Cooper and I were the first up Mirkwood on the...   \n",
       "28     6.240907e+08  The Ski.I.A investigating the lack of skiers i...   \n",
       "29     6.240907e+08  Pre-whiskey distillation laps monarchmountain!...   \n",
       "...             ...                                                ...   \n",
       "12756           NaN                                                NaN   \n",
       "12757           NaN                                                NaN   \n",
       "12758           NaN                                                NaN   \n",
       "12759  1.919449e+08                                                NaN   \n",
       "12760  2.193182e+08  Playing at Blu's tonight feeling from 8-10pm#n...   \n",
       "12761  2.193182e+08  I'm at Blu's tonight from 8-10pm!! @ Blu's Res...   \n",
       "12762  8.537449e+08  If you wanna see what shit hits the fan after ...   \n",
       "12763  1.566697e+07  Off to climb a mountain! #colorado #14er http:...   \n",
       "12764           NaN                                                NaN   \n",
       "12765  6.156375e+07                                                NaN   \n",
       "12766  1.558359e+09  Winter is coming! @ Copper Mountain Ski Area h...   \n",
       "12767  2.407886e+08  Spent Saturday at #CopperMountain with some sn...   \n",
       "12768  2.407886e+08  Blue bird day @coppermtn with mountainboy95 #v...   \n",
       "12769           NaN                                                NaN   \n",
       "12770  2.407886e+08                                                NaN   \n",
       "12771  7.041670e+07  The babes and I in Copper for mac&amp;cheese f...   \n",
       "12772  7.041670e+07  Charlotte, Freya, and I enjoying sunshine, mac...   \n",
       "12773  7.041670e+07  Mac and cheese! Hints of green chile, smoked b...   \n",
       "12774  7.041670e+07       ❤️ @ Copper Mountain https://t.co/7ui2r73fMI   \n",
       "12775  2.407886e+08  Yesterday was a good time to skip some class, ...   \n",
       "12776           NaN                                                NaN   \n",
       "12777  2.407886e+08                                                NaN   \n",
       "12778  4.204670e+08  Impact @ Blue Lakes, Colorado https://t.co/J7d...   \n",
       "12779  1.154097e+08  When your living life as an adventure 10,000 f...   \n",
       "12780  8.641441e+17  \"We sat in front of probability the two dumbes...   \n",
       "12781  1.395663e+08                                                NaN   \n",
       "12782  2.977403e+08  Exploring nature with the family round 2 #Scen...   \n",
       "12783  3.891358e+07  Getting to enjoy a little vaca in the Colorado...   \n",
       "12784  8.657139e+17  \"Rad picture from our friend Audrey at Spring ...   \n",
       "12785  1.496397e+09                                                NaN   \n",
       "\n",
       "          ...                                           description:  \\\n",
       "0         ...      \"I'm an avid outdoorsman originally from Sprin...   \n",
       "1         ...      \"be patient. work hard. stay humble. NWU soccer.\"   \n",
       "2         ...      \"An epic failure who has received a love that ...   \n",
       "3         ...      \"Father, outdoor enthusiast, food and beverage...   \n",
       "4         ...      \"Father, outdoor enthusiast, food and beverage...   \n",
       "5         ...      \"Father, outdoor enthusiast, food and beverage...   \n",
       "6         ...      \"Father, outdoor enthusiast, food and beverage...   \n",
       "7         ...      \"Father, outdoor enthusiast, food and beverage...   \n",
       "8         ...      \"Father, outdoor enthusiast, food and beverage...   \n",
       "9         ...      \"Father, outdoor enthusiast, food and beverage...   \n",
       "10        ...      \"Avid adventurer of all summer and winter spor...   \n",
       "11        ...      \"Father, outdoor enthusiast, food and beverage...   \n",
       "12        ...      \"Father, outdoor enthusiast, food and beverage...   \n",
       "13        ...                                                     \"\"   \n",
       "14        ...      \"Retired Air Force officer who runs lots of ma...   \n",
       "15        ...      \"Doing my best to tell people God is not mad a...   \n",
       "16        ...      \"I'm an avid outdoorsman originally from Sprin...   \n",
       "17        ...      \"We are located in Central Colorado in the hea...   \n",
       "18        ...      \"We are located in Central Colorado in the hea...   \n",
       "19        ...      \"We are located in Central Colorado in the hea...   \n",
       "20        ...      \"We are located in Central Colorado in the hea...   \n",
       "21        ...      \"We are located in Central Colorado in the hea...   \n",
       "22        ...      \"We are located in Central Colorado in the hea...   \n",
       "23        ...      \"We are located in Central Colorado in the hea...   \n",
       "24        ...      \"Father, outdoor enthusiast, food and beverage...   \n",
       "25        ...      \"We are located in Central Colorado in the hea...   \n",
       "26        ...      \"We are located in Central Colorado in the hea...   \n",
       "27        ...      \"We are located in Central Colorado in the hea...   \n",
       "28        ...      \"We are located in Central Colorado in the hea...   \n",
       "29        ...      \"We are located in Central Colorado in the hea...   \n",
       "...       ...                                                    ...   \n",
       "12756     ...                                                    NaN   \n",
       "12757     ...                                                    NaN   \n",
       "12758     ...                                                    NaN   \n",
       "12759     ...                       \"Greasy fingers on the keyboard\"   \n",
       "12760     ...                                             \"Musician\"   \n",
       "12761     ...                                             \"Musician\"   \n",
       "12762     ...                                         \"SEU20 ⚽️ #15\"   \n",
       "12763     ...      \"Day job with @ErgonBike. Play job with @TeamT...   \n",
       "12764     ...                                                    NaN   \n",
       "12765     ...      \"Cook, Travel, Marketer @NextGen Healthcare, B...   \n",
       "12766     ...      \"Accomplished freelance CFO, strategic consult...   \n",
       "12767     ...                                                     \"\"   \n",
       "12768     ...                                                     \"\"   \n",
       "12769     ...                                                    NaN   \n",
       "12770     ...                                                     \"\"   \n",
       "12771     ...      \"Whiskey shooter. Hopeless dreamer. Midnight w...   \n",
       "12772     ...      \"Whiskey shooter. Hopeless dreamer. Midnight w...   \n",
       "12773     ...      \"Whiskey shooter. Hopeless dreamer. Midnight w...   \n",
       "12774     ...      \"Whiskey shooter. Hopeless dreamer. Midnight w...   \n",
       "12775     ...                                                     \"\"   \n",
       "12776     ...                                                    NaN   \n",
       "12777     ...                                                     \"\"   \n",
       "12778     ...       \"always down for an adventure. insta: rathbuner\"   \n",
       "12779     ...      \"Game Changer ✨ Rule Breaker | Biz Development...   \n",
       "12780     ...                                                    NaN   \n",
       "12781     ...                                                    NaN   \n",
       "12782     ...      \"Snapchat: Newskitron   Train to become the ve...   \n",
       "12783     ...      \"Singing and a picking in the Old Crow Medicin...   \n",
       "12784     ...                                                    NaN   \n",
       "12785     ...                                                    NaN   \n",
       "\n",
       "                   created_at:.1       screen_name: geo_enabled: lang:.1  \\\n",
       "0      2016-10-09 02:38:13+00:00      \"CragSquatch\"         True    \"en\"   \n",
       "1      2017-06-21 15:01:11+00:00           \"rymian\"         True    \"en\"   \n",
       "2      2017-06-25 03:14:41+00:00     \"revrickydean\"         True    \"en\"   \n",
       "3      2017-05-04 03:43:59+00:00        \"patrickau\"         True    \"en\"   \n",
       "4      2017-05-04 10:59:04+00:00        \"patrickau\"         True    \"en\"   \n",
       "5      2017-05-14 02:56:17+00:00        \"patrickau\"         True    \"en\"   \n",
       "6      2017-05-14 12:06:47+00:00        \"patrickau\"         True    \"en\"   \n",
       "7      2017-05-14 12:11:14+00:00        \"patrickau\"         True    \"en\"   \n",
       "8      2016-12-07 01:22:00+00:00        \"patrickau\"         True    \"en\"   \n",
       "9      2017-05-04 03:42:42+00:00        \"patrickau\"         True    \"en\"   \n",
       "10     2016-01-10 23:39:34+00:00     \"MTB_RockDude\"         True    \"en\"   \n",
       "11     2016-07-28 19:14:59+00:00        \"patrickau\"         True    \"en\"   \n",
       "12     2016-08-03 11:31:48+00:00        \"patrickau\"         True    \"en\"   \n",
       "13     2013-01-08 02:07:43+00:00       \"MikeUSNRet\"         True    \"en\"   \n",
       "14     2015-03-29 18:53:42+00:00     \"yardbird1964\"         True    \"en\"   \n",
       "15     2017-06-26 01:56:02+00:00  \"josephochambers\"         True    \"en\"   \n",
       "16     2017-03-14 23:18:01+00:00      \"CragSquatch\"         True    \"en\"   \n",
       "17     2016-12-11 21:24:05+00:00  \"WoodsDistillery\"         True    \"en\"   \n",
       "18     2017-01-12 22:30:36+00:00  \"WoodsDistillery\"         True    \"en\"   \n",
       "19     2017-01-20 19:12:28+00:00  \"WoodsDistillery\"         True    \"en\"   \n",
       "20     2017-01-24 21:03:39+00:00  \"WoodsDistillery\"         True    \"en\"   \n",
       "21     2017-02-01 15:54:49+00:00  \"WoodsDistillery\"         True    \"en\"   \n",
       "22     2017-02-21 16:23:28+00:00  \"WoodsDistillery\"         True    \"en\"   \n",
       "23     2017-04-16 21:33:33+00:00  \"WoodsDistillery\"         True    \"en\"   \n",
       "24     2017-03-13 01:56:09+00:00        \"patrickau\"         True    \"en\"   \n",
       "25     2014-11-16 19:30:28+00:00  \"WoodsDistillery\"         True    \"en\"   \n",
       "26     2014-12-17 20:58:59+00:00  \"WoodsDistillery\"         True    \"en\"   \n",
       "27     2014-12-17 21:02:00+00:00  \"WoodsDistillery\"         True    \"en\"   \n",
       "28     2014-11-25 22:35:15+00:00  \"WoodsDistillery\"         True    \"en\"   \n",
       "29     2014-12-04 17:13:52+00:00  \"WoodsDistillery\"         True    \"en\"   \n",
       "...                          ...                ...          ...     ...   \n",
       "12756                        NaN                NaN          NaN     NaN   \n",
       "12757                        NaN                NaN          NaN     NaN   \n",
       "12758                        NaN                NaN          NaN     NaN   \n",
       "12759  2010-09-17 19:32:56+00:00       \"be_weeeeee\"         True    \"en\"   \n",
       "12760  2017-09-15 00:09:20+00:00       \"nsteingart\"         True    \"en\"   \n",
       "12761  2017-09-17 00:24:16+00:00       \"nsteingart\"         True    \"en\"   \n",
       "12762  2017-06-18 21:47:17+00:00   \"kayleightoupal\"         True    \"en\"   \n",
       "12763  2015-03-21 16:32:51+00:00      \"JeffKerkove\"         True    \"en\"   \n",
       "12764  2016-06-04 19:47:45+00:00                NaN          NaN     NaN   \n",
       "12765  2009-07-30 18:42:05+00:00     \"lawrencester\"         True    \"en\"   \n",
       "12766  2017-09-17 20:45:31+00:00        \"TheFitCFO\"         True    \"en\"   \n",
       "12767  2015-02-02 22:09:33+00:00          \"xxpl0re\"         True    \"en\"   \n",
       "12768  2015-03-16 00:38:48+00:00          \"xxpl0re\"         True    \"en\"   \n",
       "12769  2015-01-04 17:34:45+00:00                NaN          NaN     NaN   \n",
       "12770  2011-01-20 19:03:42+00:00          \"xxpl0re\"         True    \"en\"   \n",
       "12771  2016-07-17 02:28:24+00:00      \"ohthatbagel\"         True    \"en\"   \n",
       "12772  2016-07-17 02:32:15+00:00      \"ohthatbagel\"         True    \"en\"   \n",
       "12773  2016-07-17 02:37:20+00:00      \"ohthatbagel\"         True    \"en\"   \n",
       "12774  2016-07-17 02:51:27+00:00      \"ohthatbagel\"         True    \"en\"   \n",
       "12775  2016-04-08 15:26:51+00:00          \"xxpl0re\"         True    \"en\"   \n",
       "12776  2015-10-20 14:24:29+00:00                NaN          NaN     NaN   \n",
       "12777  2011-01-20 19:03:42+00:00          \"xxpl0re\"         True    \"en\"   \n",
       "12778  2017-05-17 03:24:04+00:00     \"joelrathbun1\"         True    \"en\"   \n",
       "12779  2017-05-15 17:30:18+00:00    \"denisebuchman\"         True    \"en\"   \n",
       "12780  2017-05-15 15:43:11+00:00                NaN          NaN    \"en\"   \n",
       "12781  2010-05-03 01:21:54+00:00         \"SimonJC_\"         True    \"en\"   \n",
       "12782  2017-05-22 23:44:35+00:00         \"J_NewSki\"         True    \"en\"   \n",
       "12783  2017-05-16 01:46:31+00:00      \"chancemccoy\"         True    \"en\"   \n",
       "12784  2017-05-19 23:40:58+00:00                NaN          NaN    \"en\"   \n",
       "12785  2013-06-09 19:12:19+00:00    \"The_Moon_Life\"         True    \"en\"   \n",
       "\n",
       "                          time_zone:                   name:  \\\n",
       "0           \"Atlantic Time (Canada)\"       \"Alec Villarreal\"   \n",
       "1       \"Central Time (US & Canada)\"         \"ryan anderson\"   \n",
       "2         Central Time (US & Canada)           \"Ricky Jones\"   \n",
       "3      \"Mountain Time (US & Canada)\"         \"Patrick Payne\"   \n",
       "4      \"Mountain Time (US & Canada)\"         \"Patrick Payne\"   \n",
       "5      \"Mountain Time (US & Canada)\"         \"Patrick Payne\"   \n",
       "6      \"Mountain Time (US & Canada)\"         \"Patrick Payne\"   \n",
       "7      \"Mountain Time (US & Canada)\"         \"Patrick Payne\"   \n",
       "8      \"Mountain Time (US & Canada)\"         \"Patrick Payne\"   \n",
       "9      \"Mountain Time (US & Canada)\"         \"Patrick Payne\"   \n",
       "10      \"Pacific Time (US & Canada)\"        \"Andrew Elliott\"   \n",
       "11     \"Mountain Time (US & Canada)\"         \"Patrick Payne\"   \n",
       "12     \"Mountain Time (US & Canada)\"         \"Patrick Payne\"   \n",
       "13                               NaN        \"Michael Wenman\"   \n",
       "14       Mountain Time (US & Canada)         \"Steve Bremner\"   \n",
       "15        Pacific Time (US & Canada)          \"Joe Chambers\"   \n",
       "16          \"Atlantic Time (Canada)\"       \"Alec Villarreal\"   \n",
       "17                           Arizona  \"Wood's HM Distillery\"   \n",
       "18                           Arizona  \"Wood's HM Distillery\"   \n",
       "19                           Arizona  \"Wood's HM Distillery\"   \n",
       "20                           Arizona  \"Wood's HM Distillery\"   \n",
       "21                           Arizona  \"Wood's HM Distillery\"   \n",
       "22                           Arizona  \"Wood's HM Distillery\"   \n",
       "23                           Arizona  \"Wood's HM Distillery\"   \n",
       "24     \"Mountain Time (US & Canada)\"         \"Patrick Payne\"   \n",
       "25                           Arizona  \"Wood's HM Distillery\"   \n",
       "26                           Arizona  \"Wood's HM Distillery\"   \n",
       "27                           Arizona  \"Wood's HM Distillery\"   \n",
       "28                           Arizona  \"Wood's HM Distillery\"   \n",
       "29                           Arizona  \"Wood's HM Distillery\"   \n",
       "...                              ...                     ...   \n",
       "12756                            NaN                     NaN   \n",
       "12757                            NaN                     NaN   \n",
       "12758                            NaN                     NaN   \n",
       "12759                            NaN              \"Ben West\"   \n",
       "12760     Central Time (US & Canada)   \"steingart@gmail.com\"   \n",
       "12761     Central Time (US & Canada)   \"steingart@gmail.com\"   \n",
       "12762                            NaN       \"Kayleigh Toupal\"   \n",
       "12763  \"Mountain Time (US & Canada)\"          \"Jeff Kerkove\"   \n",
       "12764                            NaN                     NaN   \n",
       "12765                            NaN        \"catie lawrence\"   \n",
       "12766  \"Mountain Time (US & Canada)\"          \"Mark Johnson\"   \n",
       "12767                  America/Boise        \"Kyle Personett\"   \n",
       "12768                  America/Boise        \"Kyle Personett\"   \n",
       "12769                            NaN                     NaN   \n",
       "12770                  America/Boise        \"Kyle Personett\"   \n",
       "12771     Eastern Time (US & Canada)              \"Bethanie\"   \n",
       "12772     Eastern Time (US & Canada)              \"Bethanie\"   \n",
       "12773     Eastern Time (US & Canada)              \"Bethanie\"   \n",
       "12774     Eastern Time (US & Canada)              \"Bethanie\"   \n",
       "12775                  America/Boise        \"Kyle Personett\"   \n",
       "12776                            NaN                     NaN   \n",
       "12777                  America/Boise        \"Kyle Personett\"   \n",
       "12778                            NaN          \"Joel Rathbun\"   \n",
       "12779     Eastern Time (US & Canada)     \"Denise Buchman ☕️\"   \n",
       "12780                            NaN                     NaN   \n",
       "12781     Pacific Time (US & Canada)      \"Simon John Cheng\"   \n",
       "12782   \"Central Time (US & Canada)\"        \"James Pinewski\"   \n",
       "12783                            NaN          \"Chance McCoy\"   \n",
       "12784                            NaN                     NaN   \n",
       "12785                            NaN         \"The Moon Life\"   \n",
       "\n",
       "                         location:  latitude:  longitude:  \n",
       "0                    \"Durango, CO\"  38.595941 -106.199650  \n",
       "1                               \"\"  38.596673 -106.196988  \n",
       "2                      \"Tulsa, OK\"  38.596673 -106.196988  \n",
       "3               \"Salida, Colorado\"  38.596673 -106.196988  \n",
       "4               \"Salida, Colorado\"  38.596673 -106.196988  \n",
       "5               \"Salida, Colorado\"  38.596673 -106.196988  \n",
       "6               \"Salida, Colorado\"  38.596673 -106.196988  \n",
       "7               \"Salida, Colorado\"  38.596673 -106.196988  \n",
       "8               \"Salida, Colorado\"  38.594561 -106.198577  \n",
       "9               \"Salida, Colorado\"  38.594561 -106.198577  \n",
       "10               \"Buena Vista, CO\"  38.623607 -106.330580  \n",
       "11              \"Salida, Colorado\"  38.623607 -106.330580  \n",
       "12              \"Salida, Colorado\"  38.623607 -106.330580  \n",
       "13         \"Buena Vista, Colorado\"  38.516421 -106.077239  \n",
       "14     \"Manitou Springs, Colorado\"  38.539595 -106.188343  \n",
       "15               \"Buena Vista, CO\"  38.521827 -106.078961  \n",
       "16                   \"Durango, CO\"  38.520637 -106.079676  \n",
       "17           \"144 W 1st Salida Co\"  38.520637 -106.079676  \n",
       "18           \"144 W 1st Salida Co\"  38.520637 -106.079676  \n",
       "19           \"144 W 1st Salida Co\"  38.520637 -106.079676  \n",
       "20           \"144 W 1st Salida Co\"  38.520637 -106.079676  \n",
       "21           \"144 W 1st Salida Co\"  38.520637 -106.079676  \n",
       "22           \"144 W 1st Salida Co\"  38.520637 -106.079676  \n",
       "23           \"144 W 1st Salida Co\"  38.520637 -106.079676  \n",
       "24              \"Salida, Colorado\"  38.520637 -106.079676  \n",
       "25           \"144 W 1st Salida Co\"  38.536800 -106.180128  \n",
       "26           \"144 W 1st Salida Co\"  38.536800 -106.180128  \n",
       "27           \"144 W 1st Salida Co\"  38.536800 -106.180128  \n",
       "28           \"144 W 1st Salida Co\"  38.534321 -106.144861  \n",
       "29           \"144 W 1st Salida Co\"  38.534321 -106.144861  \n",
       "...                            ...        ...         ...  \n",
       "12756                          NaN        NaN         NaN  \n",
       "12757                          NaN        NaN         NaN  \n",
       "12758                          NaN        NaN         NaN  \n",
       "12759                 \"Dillon, CO\"  39.502002 -106.150997  \n",
       "12760                  \"Vail, CO \"  39.631029 -106.290523  \n",
       "12761                  \"Vail, CO \"  39.631029 -106.290523  \n",
       "12762                           \"\"  39.630966 -106.290313  \n",
       "12763                  \"Eagle, CO\"  39.535257 -106.142588  \n",
       "12764                          NaN        NaN         NaN  \n",
       "12765                  \"Colorado \"  39.543088 -106.119953  \n",
       "12766                   \"Colorado\"  39.500779 -106.153743  \n",
       "12767                   \"Colorado\"  39.500385 -106.156559  \n",
       "12768                   \"Colorado\"  39.500382 -106.156582  \n",
       "12769                          NaN        NaN         NaN  \n",
       "12770                   \"Colorado\"  39.500151 -106.155966  \n",
       "12771                 \"Denver, CO\"  39.500227 -106.156081  \n",
       "12772                 \"Denver, CO\"  39.500227 -106.156081  \n",
       "12773                 \"Denver, CO\"  39.500227 -106.156081  \n",
       "12774                 \"Denver, CO\"  39.500227 -106.156081  \n",
       "12775                   \"Colorado\"  39.500379 -106.156654  \n",
       "12776                          NaN        NaN         NaN  \n",
       "12777                   \"Colorado\"  39.500384 -106.156596  \n",
       "12778                    \"Wichita\"  39.383879 -106.068049  \n",
       "12779              \"United States\"  39.479822 -106.045828  \n",
       "12780                          NaN        NaN         NaN  \n",
       "12781          \"Las Vegas, Nevada\"  39.184329 -105.963198  \n",
       "12782     \"Pallet Town, Minnesota\"  39.577744 -106.129562  \n",
       "12783              \"Nashville, TN\"  38.842391 -106.119731  \n",
       "12784                          NaN        NaN         NaN  \n",
       "12785                           \"\"  38.843933 -106.123290  \n",
       "\n",
       "[12786 rows x 23 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"./parsed_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Load the csv data as a panda dataframe\n",
    "tweets_df = pd.read_csv(\"./parsed_tweets.csv\")\n",
    "\n",
    "#4 Create rdd of the text of every tweet\n",
    "tweetrdd =  sc.parallelize(tweets_df['text:'], 10)\\\n",
    "              .filter(lambda x: type(x) == str)\\\n",
    "              .map(lambda tweet: re.sub(('\\n'), '', tweet))\\\n",
    "              .map(lambda tweet: tweet.split(' @ ', )[0])\\\n",
    "              .map(lambda tweetwords: re.sub('https?', '', tweetwords))\\\n",
    "            .map(lambda tweetwords: re.sub(' co', '', tweetwords))\\\n",
    "\n",
    "#5 Set the n_oftweets to analyze and create a subset rdd out of the original tweet texts\n",
    "n_oftweets = 900\n",
    "tweetrddsubset = sc.parallelize(tweetrdd.take(n_oftweets))\n",
    "\n",
    "#6 Partition the tweetrdd into a list of words per individual tweet and filter out the special characters&stopwords\n",
    "stopwords = [\"ourselves\", \"hers\", \"between\", \"yourself\", \"but\", \"again\", \"there\", \"about\", \"once\", \"during\", \"out\", \"very\", \"having\", \"with\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\", \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\", \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"this\", \"down\", \"should\", \"our\", \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \"ours\", \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\", \"them\", \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\", \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"not\", \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\", \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\", \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\", \"how\", \"further\", \"was\", \"here\", \"than\"]\n",
    "wordrdd_tweet = tweetrddsubset.map(lambda line: [re.findall('\\w*', word.lower())[0] for word in line.split(' ')] )\\\n",
    "                        .map(lambda tweet: list(filter(lambda tweet: tweet is not '', tweet)))\\\n",
    "                        .map(lambda wordlist: [word for word in wordlist if word not in stopwords])\n",
    "            \n",
    "#7 Costruct the rdd of all words of all tweets together\n",
    "wordrdd = tweetrddsubset.flatMap(lambda line: line.split())\\\n",
    "                  .map(lambda word: word.lower())\\\n",
    "                  .map(lambda word: re.findall('\\w*' , word)[0])\\\n",
    "                  .filter(lambda word: word is not '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compute the dissimilarity between every [*tweet with every tweet*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 count the number of words as a summary statistic\n",
    "total_wordcount = wordrdd.count()\n",
    "\n",
    "#2 Count the number of times the word repeats within the same tweet for all tweets\n",
    "countedwordpairrdd_tweet = wordrdd_tweet.map(lambda tweet: sorted(tweet))\\\n",
    "                                    .map(lambda tweetsorted: [(word,len(list(n))) for word,n in groupby(tweetsorted)])\n",
    "\n",
    "#3 Calculate the term frequency (TF) per tweet as RDD \n",
    "tf_tweet = countedwordpairrdd_tweet.map(lambda wordpair: [(word, n/len(wordpair)) for word, n in wordpair])\n",
    "\n",
    "#4 Create a unique word rdd and count the number of words as a summary statistic\n",
    "wordrdd_unique = wordrdd.map(lambda word: (word,1))\\\n",
    "                        .reduceByKey(lambda key, next_key : key + next_key)\\\n",
    "                        .map(lambda wordpair: wordpair[0])\n",
    "\n",
    "n_uniquewords = wordrdd_unique.count()\n",
    "\n",
    "#5 Convert&collect uniquewordrdd to K-V pair list with zeros to append to every tweet to equalize tweetsize later on\n",
    "uniquewordvector_withzeros = wordrdd_unique.map(lambda word: (word, 0)).collect()\n",
    "\n",
    "#6 Remove duplicate words per tweet and count total number of tweets each word occurs in\n",
    "idf  = countedwordpairrdd_tweet.map(lambda tweet: [(word, 1) for word,n in tweet])\\\n",
    "                               .flatMap(lambda tweet: tweet)\\\n",
    "                               .reduceByKey(lambda value_so_far, next_value : value_so_far + next_value)\\\n",
    "                               .map(lambda wordpair: (wordpair[0], log10(n_oftweets/wordpair[1])))              \n",
    "\n",
    "#7 Collect the IDF of all unique words as a dictionary\n",
    "idfdict = idf.collectAsMap()\n",
    "\n",
    "#8 Create TFIDF by multiplying TF of every word per tweet with the lookedup IDF value from the IDF dictionary above\n",
    "tfidf_word_tweet = tf_tweet.map(lambda wordpair: [ (word, tf*idfdict[word]) for word,tf in wordpair ] )\n",
    "\n",
    "#9 Equalize every tweet's length by inserting missing unique wordpairs created in step #12, zip result with index\n",
    "total_indexedtfidfmatrix = tfidf_word_tweet.map(lambda tweetwordpairs: tweetwordpairs + list(filter(lambda word: word[0] not in [word for word, tfidf in tweetwordpairs], uniquewordvector_withzeros)))\\\n",
    "                                           .map(lambda tweet: sorted(tweet, key=lambda x: x[0]))\\\n",
    "                                           .map(lambda wordvector: [tfidf for word, tfidf in wordvector])\\\n",
    "                                           .map(lambda tfidfvector: Vectors.dense([tfidf for tfidf in tfidfvector]))\\\n",
    "                                           .zipWithIndex()      \n",
    "\n",
    "# 10 Create a matrix that uses the cartesian product between every tweet to construct tweet comparison containers.\n",
    "# Then compute the cosine dissimilarity between every such container to get a measure of distance between each tweet \n",
    "dissimilarity_rdd = total_indexedtfidfmatrix.cartesian(total_indexedtfidfmatrix)\\\n",
    "                                            .sortBy(lambda pairof_vectorindexpairs: (pairof_vectorindexpairs[0][1],  pairof_vectorindexpairs[1][1]) )\\\n",
    "                                            .map(lambda pairof_vectorindexpairs: (1-(pairof_vectorindexpairs[0][0].dot(pairof_vectorindexpairs[1][0])/(pairof_vectorindexpairs[0][0].norm(2)*pairof_vectorindexpairs[1][0].norm(2))),  (pairof_vectorindexpairs[0][1], pairof_vectorindexpairs[1][1]) ))\\\n",
    "                                            .map(lambda tweetsimilaritypair: (1.0, tweetsimilaritypair[1]) if isnan(tweetsimilaritypair[0]) else tweetsimilaritypair)\n",
    "\n",
    "# 11 Compartamentalize every entry of the dissimilarityrdd per tweet to construct indexed matrix that compares each                   \n",
    "# tweet with every other tweet (watch out this results in a very LARGE MATRIX!)\n",
    "dissimilarity_matrix = dissimilarity_rdd.groupBy(lambda x: x[1][0])\\\n",
    "                                        .map(lambda tweetvsrest: list(tweetvsrest[1]))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3  Compute the dissimilarity vectors of  [*co-ocurring words*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Construct a term frequency matrix for every word instead of every tweet\n",
    "total_indexedtfmatrix = tf_tweet.map(lambda tweetwordpairs: tweetwordpairs + list(filter(lambda word: word[0] not in [word for word, tfidf in tweetwordpairs], uniquewordvector_withzeros)))\\\n",
    "                                .map(lambda tweet: sorted(tweet, key=lambda x: x[0]))\\\n",
    "                                .flatMap(lambda x: x)\\\n",
    "                                .map(lambda x: (x[0],[x[1]]))\\\n",
    "                                .reduceByKey(lambda x, y: x+y)\\\n",
    "                                .map(lambda wordvector: wordvector[1])\\\n",
    "                                .map(lambda tf_wordvector: Vectors.dense([tf for tf in tf_wordvector]))\\\n",
    "                                .zipWithIndex()  \n",
    "\n",
    "#2 Calculate the dissimilarity between every word                \n",
    "dissimilarity_rdd_word = total_indexedtfmatrix.cartesian(total_indexedtfmatrix)\\\n",
    "                                                .sortBy(lambda pairof_vectorindexpairs: (pairof_vectorindexpairs[0][1],  pairof_vectorindexpairs[1][1]) )\\\n",
    "                                                .map(lambda pairof_vectorindexpairs: (1-(pairof_vectorindexpairs[0][0].dot(pairof_vectorindexpairs[1][0])/(pairof_vectorindexpairs[0][0].norm(2)*pairof_vectorindexpairs[1][0].norm(2))),  (pairof_vectorindexpairs[0][1], pairof_vectorindexpairs[1][1]) ))\\\n",
    "                                                .map(lambda wordsimilaritypair: (1.0, wordsimilaritypair[1]) if isnan(wordsimilaritypair[0]) else wordsimilaritypair)\n",
    "\n",
    "# Make a matrix out of every word vs every other word            \n",
    "dissimilarity_wordmatrix = dissimilarity_rdd_word.groupBy(lambda x: x[1][0])\\\n",
    "                                                 .map(lambda wordvsrest: list(wordvsrest[1]))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (opt). Print summary statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of totals with line below\n",
    "print('total words:', total_wordcount, ', unique words:', n_uniquewords, ', number of tweets:', n_oftweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Cluster and plot the [*co-ocurring words*] dissimilarity cluster costs with K-Means using different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the word dissimilarity data into a dataframe\n",
    "Data = dissimilarity_wordmatrix.map(lambda similarityvector: array([similarityentry[0] for similarityentry in similarityvector]))\\\n",
    "                           .map(lambda x : (Vectors.dense(x),))\n",
    "df = spark.createDataFrame(Data, [\"features\"])\n",
    "\n",
    "# 2. Loop through all models \n",
    "k = range(2,12,2)\n",
    "cost_all = []\n",
    "\n",
    "for i in k:\n",
    "    bkm = BisectingKMeans(k=i).setSeed(1)\n",
    "    model = bkm.fit(df)\n",
    "    cost = model.computeCost(df)###type(cost) is float\n",
    "    cost_all = cost_all+[cost]\n",
    "\n",
    "# 3. Plot costs vs clusters \n",
    "plt.plot(k,cost_all)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('K-cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Choose optimal model for [*co-ocurring words*] clusters and print the words within these clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_final = 8\n",
    "\n",
    "# Trains a bisecting k-means model.\n",
    "bkm = BisectingKMeans().setK(k_final).setSeed(1)\n",
    "model = bkm.fit(df)\n",
    "prediction = model.transform(df).select('prediction').collect()\n",
    "labels = [p.prediction for p in prediction ]\n",
    "\n",
    "# Evaluate clustering.\n",
    "cost = model.computeCost(df)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(cost))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Print the top results of the [*co-ocurring words*] per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderedwordrdd_unique = sorted(wordrdd_unique.collect())\n",
    "\n",
    "#0. Set a threshold for the minimum number of occurences \n",
    "threshold = 4\n",
    "\n",
    "#1. Calculate the number of word occurs per tweet\n",
    "wordcounts_tweet = countedwordpairrdd_tweet.map(lambda wordpair: [(word, n) for word, n in wordpair])\n",
    "\n",
    "#2. Create a dictionary to look up the total number of document occurences each word has\n",
    "noftweet_worddict = wordcounts_tweet.map(lambda tweetwordpairs: tweetwordpairs + list(filter(lambda word: word[0] not in [word for word, tfidf in tweetwordpairs], uniquewordvector_withzeros)))\\\n",
    "                                    .map(lambda tweet: sorted(tweet, key=lambda x: x[0]))\\\n",
    "                                    .flatMap(lambda x: x)\\\n",
    "                                    .map(lambda x: (x[0],[x[1]]))\\\n",
    "                                    .reduceByKey(lambda x, y: x+y)\\\n",
    "                                    .map(lambda pair: (pair[0], reduce(lambda x, y: x+y, pair[1]) ))\\\n",
    "                                    .collectAsMap()\n",
    "\n",
    "#3. Divide all the words in each word cluster with their number of total document occurences \n",
    "wordclusters = sc.parallelize(labels)\\\n",
    "                 .zipWithIndex()\\\n",
    "                 .map(lambda pair: (pair[0], ((orderedwordrdd_unique[pair[1]]), noftweet_worddict[orderedwordrdd_unique[pair[1]]])))\\\n",
    "                 .map(lambda x: (x[0], [x[1]]) )\\\n",
    "                 .reduceByKey(lambda x,y: x+y)\\\n",
    "                 .map(lambda cluster: (cluster[0], sorted(cluster[1], key=lambda list: -list[1] )))\n",
    "\n",
    "#4. Pick out the top words per cluster with the preset threshold in 0\n",
    "topwordclusters = wordclusters.map(lambda cluster: (cluster[0], [x for x,y in cluster[1] if y >= threshold])).collect()\n",
    "\n",
    "\n",
    "#5. Print out the results\n",
    "print('Most representative words for each cluster:')\n",
    "number = 0\n",
    "\n",
    "for cluster in topwordclusters:\n",
    "    print (number, '\\n', ', '.join(cluster[1]))\n",
    "    number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Plot histograms of the clusters of [*co-ocurring words*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Plot the histogram of the words vs number of clusters\n",
    "plt.hist(labels, facecolor='purple')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Number of word')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Cluster and plot the [*tweet dissimilarity*] vector matrix with K-Means with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the tweet dissimilarity data into a dataframe\n",
    "Data = dissimilarity_matrix.map(lambda similarityvector: array([similarityentry[0] for similarityentry in similarityvector]))\\\n",
    "                           .map(lambda x : (Vectors.dense(x),))\n",
    "df = spark.createDataFrame(Data, [\"features\"])\n",
    "\n",
    "# 2. Loop through all models \n",
    "k = range(2,10,2)\n",
    "cost_all = []\n",
    "\n",
    "for i in k:\n",
    "    bkm = BisectingKMeans(k=i).setSeed(1)\n",
    "    model = bkm.fit(df)\n",
    "    cost = model.computeCost(df)\n",
    "    cost_all = cost_all+[cost]\n",
    "\n",
    "# 3. Plot costs vs clusters \n",
    "plt.plot(k,cost_all)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('K-cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Choose the optimal model for the [*tweet*] clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_final = 6\n",
    "\n",
    "# 1. Trains a bisecting k-means model.\n",
    "bkm = BisectingKMeans().setK(k_final).setSeed(1)\n",
    "final_model = bkm.fit(df)\n",
    "prediction = final_model.transform(df).select('prediction').collect()\n",
    "labels = [p.prediction for p in prediction ]\n",
    "\n",
    "# 2. Evaluate clustering.\n",
    "cost = final_model.computeCost(df)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(cost))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Couple all [*tweets*] with cluster and analyze the TFIDF for every word that falls in a tweet cluster. Then, retrieve the top 10 distinctive words for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Create tweetclusterindices\n",
    "tweetclusterindices = sc.parallelize(labels)\\\n",
    "                        .zipWithIndex()\\\n",
    "                        .map(lambda pair: (pair[1], pair[0]))\\\n",
    "                        .collect()\n",
    "\n",
    "\n",
    "# 1 Merge all tweets in cluster\n",
    "words_clusterrdd = wordrdd_tweet.map(lambda cluster: ' '.join(cluster))\\\n",
    "                                 .zipWithIndex()\\\n",
    "                                 .map(lambda pair: (pair[0], tweetclusterindices[pair[1]]))\\\n",
    "                                 .map(lambda pair: (pair[0], pair[1][1]))\\\n",
    "                                 .groupBy(lambda pair: pair[1])\\\n",
    "                                 .map(lambda pair: list(pair[1]))\\\n",
    "                                 .map(lambda cluster: [pair[0] for pair in cluster])\\\n",
    "                                 .map(lambda cluster: [' '.join(cluster)])\\\n",
    "                                 .map(lambda cluster: cluster[0].split())\n",
    "# 2 Count word pair                                  \n",
    "countedwordpairrdd_cluster = words_clusterrdd.map(lambda tweet: sorted(tweet))\\\n",
    "                                    .map(lambda tweetsorted: [(word,len(list(n))) for word,n in groupby(tweetsorted)])\n",
    "\n",
    "#3 Calculate the term frequency (TF) per cluster\n",
    "tf_cluster = countedwordpairrdd_cluster.map(lambda wordpair: [(word, n/len(wordpair)) for word, n in wordpair])\n",
    "\n",
    "#4 Remove duplicate words per cluster and count total number of clusters each word occurs in\n",
    "idf_cluster  = countedwordpairrdd_cluster.map(lambda tweet: [(word, 1) for word,n in tweet])\\\n",
    "                               .flatMap(lambda tweet: tweet)\\\n",
    "                               .reduceByKey(lambda value_so_far, next_value : value_so_far + next_value)\\\n",
    "                               .map(lambda wordpair: (wordpair[0], log10(k_final/wordpair[1]))) \n",
    "\n",
    "#5 Collect the IDF of all unique words as a dictionary\n",
    "idfdictcluster = idf.collectAsMap()\n",
    "\n",
    "#6 Create TFIDF by multiplying TF of every word per cluster with the IDF value from the IDF dictionary above\n",
    "tfidf_word_cluster = tf_cluster.map(lambda wordpair: [ (word, tf*idfdictcluster[word]) for word,tf in wordpair ] )\n",
    "\n",
    "#7 Get representatitve words per cluster (tf and tfidf)\n",
    "sortedtfidf_word_cluster = tfidf_word_cluster.map(lambda cluster: [b for a,b in sorted((-tup[1], tup) for tup in cluster)] )\n",
    "sortedtf_word_cluster = tf_cluster.map(lambda cluster: [b for a,b in sorted((-tup[1], tup) for tup in cluster)] )\n",
    "\n",
    "#8 Get representatitve words per cluster\n",
    "tfidf_representative_words_cluster = [sortedtfidf_word_cluster.collect()[index][:10] for index in range(k_final)]\n",
    "toptfidf_representative_words_cluster = list(map(lambda x: [y[0] for y  in x], tfidf_representative_words_cluster))\n",
    "\n",
    "tf_representative_words_cluster = [sortedtf_word_cluster.collect()[index][:10] for index in range(k_final)]\n",
    "toptf_representative_words_cluster = list(map(lambda x: [y[0] for y  in x], tf_representative_words_cluster))\n",
    "\n",
    "print('Most representative words for each cluster with TFIDF:')\n",
    "number = 0\n",
    "for cluster in toptfidf_representative_words_cluster:\n",
    "    print (number, cluster)\n",
    "    number += 1\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('Most representative words for each cluster with TF:')\n",
    "number2 = 0\n",
    "for cluster in toptf_representative_words_cluster:\n",
    "    print (number2, cluster)\n",
    "    number2 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Create a Histogram of all the tweet clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Load the cluster information\n",
    "tweets_cluster_plot =  list(map(lambda pair: pair[1], tweetclusterindices))\n",
    "\n",
    "#2. Plot the histogram of the tweets vs the clusters\n",
    "plt.hist(tweets_cluster_plot, facecolor='purple')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Number tweets')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python with Pixiedust (Spark 2.2)",
   "language": "python",
   "name": "pythonwithpixiedustspark22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
